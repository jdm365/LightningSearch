const std     = @import("std");
const builtin = @import("builtin");

const csv  = @import("../parsing/csv.zig");
const json = @import("../parsing/json.zig");
const pq   = @import("../parsing/parquet.zig");

const SortedIntMultiArray = @import("../utils/sorted_array.zig").SortedIntMultiArray;
const CaseInsensitiveWyhash = @import("../utils/custom_wyhash.zig").CaseInsensitiveWyhash;
const CaseInsensitiveWyhashContext = @import("../utils/custom_wyhash.zig").CaseInsensitiveWyhashContext;
const string_utils = @import("../utils/string_utils.zig");
const file_utils   = @import("../storage/file_utils.zig");

const findSorted       = @import("../utils/misc_utils.zig").findSorted;
const TermPos          = @import("../server/server.zig").TermPos;
const StaticIntegerSet = @import("../utils/static_integer_set.zig").StaticIntegerSet;
const PruningRadixTrie = @import("../utils/pruning_radix_trie.zig").PruningRadixTrie;
const RadixTrie        = @import("../utils/radix_trie.zig").RadixTrie;
const DocStore         = @import("../storage/doc_store.zig").DocStore;

pub const MAX_NUM_RESULTS = 1000;
pub const MAX_TERM_LENGTH = 256;
// pub const MAX_NUM_TERMS   = 16_384;
pub const MAX_NUM_TERMS   = 65_536;
pub const MAX_LINE_LENGTH = 1_048_576;


const POST_ALIGNMENT = 64;

const K1: f32 = 1.4;
const B:  f32 = 0.75;
const C1: f32 = K1 * (1 - B);

pub inline fn scoreBM25(
    idf: f32,
    term_freq: u32,
    doc_size: u16,
    avg_doc_size: f32,
) f32 {
    const ftf: f32 = @floatFromInt(term_freq);
    const fds: f32 = @floatFromInt(doc_size);

    const num   = ftf * (K1 + 1) * idf;
    const denom = (ftf + (K1 * (1 - B + (B * fds / avg_doc_size))));

    return num / denom;
}


pub inline fn scoreBM25Fast(
    term_freq: u16, 
    doc_size: u16,
    A: f32,
    C2: f32,
    ) f32 {
    const tf: f32 = @floatFromInt(term_freq);
    const d:  f32 = @floatFromInt(doc_size);

    const L = tf + C1 + C2 * d;
    const r = 1.0 / L;
    return A * tf * r;
}

inline fn lowerBound(comptime T: type, slice: []const T, target: T) usize {
    var low: usize = 0;
    var high: usize = slice.len;

    while (low < high) {
        const mid = low + ((high - low) >> 1);
        if (slice[mid] < target) {
            low = mid + 1;
        } else {
            high = mid;
        }
    }
    return low;
}

inline fn linearLowerBound(slice: []const u32, target: u32) usize {
    const new_value: @Vector(8, u32) align(4) = @splat(target);
    var existing_values = @as(
        *const align(4) @Vector(8, u32), 
        @alignCast(@ptrCast(slice.ptr)),
        );

    const simd_limit: usize = @divFloor(slice.len, 8) + 
                              8 * @as(usize, @intFromBool(slice.len % 8 != 0));
    var idx: usize = 0;
    while (idx < simd_limit) {
        const mask = new_value < existing_values.*;

        const set_idx = @ctz(@as(u8, @bitCast(mask)));
        if (set_idx != 8) {
            return @min(idx + set_idx, slice.len);
        }

        existing_values = @ptrFromInt(@intFromPtr(existing_values) + 32);
        idx += 8;
    }
    return slice.len;
}

inline fn linearLowerBoundSIMD(
    slice: *const align(64) [BLOCK_SIZE]u32, 
    start_idx: usize,
    target: u32,
    ) ?usize {
    // We have a guarantee that the current doc
    // is less than the target, so ids in prev loaded SIMD
    // buffer shouldn't pass.
    const adjusted_start_idx = std.mem.alignBackward(
        usize,
        start_idx,
        16,
    );
    const start_block_idx = adjusted_start_idx >> 4;

    const new_value: @Vector(16, u32) = @splat(target);
    var existing_values = @as(
        *const @Vector(16, u32), 
        @alignCast(@ptrCast(slice[adjusted_start_idx..])),
        );

    var block_idx = start_block_idx;
    while (block_idx < NUM_BLOCKS) {
        const mask = new_value <= existing_values.*;

        const set_idx = @ctz(@as(u16, @bitCast(mask)));
        if (set_idx != 16) {
            return (block_idx << 4) + set_idx;
        }

        existing_values = @ptrFromInt(
            @intFromPtr(existing_values) + @sizeOf(@Vector(16, u32))
            );
        block_idx += 1;
    }
    return null;
}

pub const IteratorHeap = struct {
    its: []PostingsIteratorV2,
    current_idx: usize,

    // WIP

    pub fn init(
        allocator: std.mem.Allocator,
        iterators_arr: []PostingsIteratorV2,
        ) !IteratorHeap {
        return IteratorHeap{
            .its = try allocator.alignedAlloc(
                PostingsIteratorV2, 
                .@"16",
                iterators_arr.len,
                ),
            .current_idx = 0,
        };
    }

    pub fn deinit(
        self: IteratorHeap,
        allocator: std.mem.Allocator,
        ) void {
        allocator.free(self.its);
    }

    pub inline fn pop(self: IteratorHeap) PostingsIteratorV2 {
        const init_idx = self.current_idx;
        self.current_idx += 1;
        return self.its[init_idx];
    }

    pub inline fn push(self: IteratorHeap, new_it: PostingsIteratorV2) void {
        std.debug.assert(self.current_idx != 0);

        self.its[self.current_idx] = new_it;
        self.current_idx -= 1;

        self.heapify();
    }

    pub inline fn heapify(self: IteratorHeap) void {
        var idx = self.current_idx;
        while (idx < self.its.len) {
            const cur_it = self.its[idx];
            const parent_idx = (idx - 1) >> 1;

            if (parent_idx >= self.current_idx) {
                break;
            }

            const parent_it = self.its[parent_idx];
            if (cur_it.current_block_max_score > 
                parent_it.current_block_max_score) {

                self.its[idx] = parent_it;
                self.its[parent_idx] = cur_it;
                idx = parent_idx;
            } else {
                break;
            }
        }
    }
};

pub const PostingsIteratorV2 = struct {
    postings_list: PostingsListV2,
    term_id: u32,
    posting_len: usize,
    uncompressed_doc_ids_buffer: [BLOCK_SIZE]u32 align(64),
    uncompressed_tfs_buffer: [BLOCK_SIZE]u16 align(64),
    current_doc_idx: usize,
    tp_idx: usize,
    boost_weighted_idf: f32,
    boost: f32,

    current_block_max_score: f32,
    A: f32,
    C2: f32,

    col_idx: u32,
    avg_doc_size: f32,
    consumed: bool,
    on_partial_block: bool,
    fb_meta: PostingsBlockFullV2,

    pub const Result = packed struct(u64) {
        doc_id:    u32,
        term_freq: u16,
        term_pos:  u16,
    };

    pub fn init(
        postings_list: PostingsListV2,
        term_id: u32,
        col_idx: u32,
        idf: f32,
        boost: f32,
        avg_doc_size: f32,
        ) !PostingsIteratorV2 {
        const partial_block = &postings_list.partial.items[term_id];
        const posting_len = (BLOCK_SIZE * partial_block.num_full_blocks) + 
                             partial_block.num_docs;
        var it = PostingsIteratorV2{ 
            .postings_list = postings_list,
            .term_id = term_id,
            .posting_len = posting_len,
            .uncompressed_doc_ids_buffer =  [_]u32{0} ** BLOCK_SIZE,
            .uncompressed_tfs_buffer = [_]u16{0} ** BLOCK_SIZE,
            .current_doc_idx = 0,
            .tp_idx = undefined,
            .boost_weighted_idf = idf * boost,
            .boost = boost,

            .current_block_max_score = 0.0,
            .A = (K1 + 1) * idf * boost,
            .C2 = (K1 * B) / avg_doc_size,

            .col_idx = col_idx,
            .avg_doc_size = avg_doc_size,
            .consumed = false,
            .on_partial_block = posting_len < BLOCK_SIZE,
            .fb_meta = undefined,
        };

        if (it.on_partial_block) {
            // Decompress partial
            partial_block.decompressToBuffers(
                &it.uncompressed_doc_ids_buffer,
                &it.uncompressed_tfs_buffer,
                );
            it.current_block_max_score = scoreBM25Fast(
                partial_block.max_tf,
                partial_block.max_doc_size,
                it.A,
                it.C2,
            );
        } else {
            // Decompress full
            const start_buf_idx = it.postings_list.full_buf_idxs.items[term_id];

            std.debug.assert(it.partial().num_docs != BLOCK_SIZE);
            std.debug.assert(it.partial().num_full_blocks >= 1);
            it.fb_meta = try PostingsBlockFullV2.decompressToBuffers(
                &it.uncompressed_doc_ids_buffer,
                &it.uncompressed_tfs_buffer,
                it.postings_list.full_buffer,
                start_buf_idx,
                );
            it.current_block_max_score = scoreBM25Fast(
                it.fb_meta.max_tf,
                it.fb_meta.max_doc_size,
                it.A,
                it.C2,
            );
        }

        return it;
    }

    pub inline fn currentDocId(self: *const PostingsIteratorV2) ?u32 {
        if (self.consumed) {
            return null;
        }
        return self.uncompressed_doc_ids_buffer[self.current_doc_idx % BLOCK_SIZE];
    }

    pub inline fn currentTermFreq(self: *const PostingsIteratorV2) ?u16 {
        if (self.consumed) {
            return null;
        }
        return self.uncompressed_tfs_buffer[self.current_doc_idx % BLOCK_SIZE];
    }

    // pub inline fn currentTermPos(self: *const PostingsIteratorV2) ?u16 {
        // if (self.current_idx >= self.term_positions.len) {
            // return null;
        // }
// 
        // return self.term_positions[self.current_idx];
    // }

    pub inline fn partial(self: *PostingsIteratorV2) PostingsBlockPartial {
        return self.postings_list.partial.items[self.term_id];
    }

    pub inline fn next(self: *PostingsIteratorV2) !?Result {
        if (self.consumed) {
            return null;
        }

        const current_res = Result{
            .doc_id    = self.uncompressed_doc_ids_buffer[self.current_doc_idx % BLOCK_SIZE],
            .term_freq = self.uncompressed_tfs_buffer[self.current_doc_idx % BLOCK_SIZE],
            .term_pos  = undefined,
        };

        self.current_doc_idx += 1;

        if (self.current_doc_idx >= self.posting_len) {
            self.consumed = true;
            self.current_doc_idx = std.math.maxInt(u32);
            return null;
        }


        const partial_block = self.partial();
        if (self.current_doc_idx % BLOCK_SIZE == 0) {
            const block_idx = self.current_doc_idx >> (comptime std.math.log2(BLOCK_SIZE));

            if (block_idx == partial_block.num_full_blocks) {
                self.on_partial_block = true;
                partial_block.decompressToBuffers(
                    &self.uncompressed_doc_ids_buffer,
                    &self.uncompressed_tfs_buffer,
                    );
            } else {
                std.debug.assert(self.fb_meta.next_block_idx != std.math.maxInt(u32));
                self.fb_meta = try PostingsBlockFullV2.decompressToBuffers(
                    &self.uncompressed_doc_ids_buffer,
                    &self.uncompressed_tfs_buffer,
                    self.postings_list.full_buffer,
                    self.fb_meta.next_block_idx,
                    );
                self.current_block_max_score = scoreBM25Fast(
                    self.fb_meta.max_tf,
                    self.fb_meta.max_doc_size,
                    self.A,
                    self.C2,
                );
            }

            if (self.on_partial_block) {
                self.current_block_max_score = scoreBM25Fast(
                        partial_block.max_tf,
                        partial_block.max_doc_size,
                        self.A,
                        self.C2,
                    );
            } else {
                self.current_block_max_score = scoreBM25Fast(
                        self.fb_meta.max_tf,
                        self.fb_meta.max_doc_size,
                        self.A,
                        self.C2,
                    );
            }
        }
        // std.debug.print("Current doc id: {d}\n", .{self.uncompressed_doc_ids_buffer[self.current_doc_idx % BLOCK_SIZE]});

        return current_res;
    }

    pub inline fn nextSkipping(self: *PostingsIteratorV2, min_score: f32) !?Result {
        if (self.consumed) {
            return null;
        }

        const num_full_blocks = self.partial().num_full_blocks;
        var skipped = false;
        while (self.current_block_max_score <= min_score) {
            if (self.on_partial_block) {
                self.consumed = true;
                self.current_doc_idx = std.math.maxInt(u32);
                return null;
            }

            skipped = true;

            self.current_doc_idx = std.mem.alignBackward(
                usize,
                self.current_doc_idx + BLOCK_SIZE,
                64,
            );
            const block_idx = self.current_doc_idx >> (comptime std.math.log2(BLOCK_SIZE));
            if (block_idx == num_full_blocks) {
                self.on_partial_block = true;
            }
            continue;
        }

        const prev_res = Result{
            .doc_id    = self.uncompressed_doc_ids_buffer[self.current_doc_idx % BLOCK_SIZE],
            .term_freq = self.uncompressed_tfs_buffer[self.current_doc_idx % BLOCK_SIZE],
            .term_pos  = undefined,
        };


        const partial_block = self.partial();
        if (skipped) {
            if (self.on_partial_block) {
                partial_block.decompressToBuffers(
                    &self.uncompressed_doc_ids_buffer,
                    &self.uncompressed_tfs_buffer,
                    );
            } else {
                std.debug.assert(self.fb_meta.next_block_idx != std.math.maxInt(u32));
                self.fb_meta = try PostingsBlockFullV2.decompressToBuffers(
                    &self.uncompressed_doc_ids_buffer,
                    &self.uncompressed_tfs_buffer,
                    self.postings_list.full_buffer,
                    self.fb_meta.next_block_idx,
                    );
                self.current_block_max_score = scoreBM25Fast(
                    self.fb_meta.max_tf,
                    self.fb_meta.max_doc_size,
                    self.A,
                    self.C2,
                );
            }
        } else {
            self.current_doc_idx += 1;

            const block_idx = @divFloor(self.current_doc_idx, BLOCK_SIZE);
            if (self.current_doc_idx % BLOCK_SIZE == 0) {
                self.on_partial_block = (block_idx == num_full_blocks);
                if (self.on_partial_block) {
                    partial_block.decompressToBuffers(
                        &self.uncompressed_doc_ids_buffer,
                        &self.uncompressed_tfs_buffer,
                        );
                } else {
                    std.debug.assert(self.fb_meta.next_block_idx != std.math.maxInt(u32));
                    self.fb_meta = try PostingsBlockFullV2.decompressToBuffers(
                        &self.uncompressed_doc_ids_buffer,
                        &self.uncompressed_tfs_buffer,
                        self.postings_list.full_buffer,
                        self.fb_meta.next_block_idx,
                        );
                    self.current_block_max_score = scoreBM25Fast(
                        self.fb_meta.max_tf,
                        self.fb_meta.max_doc_size,
                        self.A,
                        self.C2,
                    );
                }
            }
        }


        if (self.current_doc_idx >= self.posting_len) {
            self.consumed = true;
            self.current_doc_idx = std.math.maxInt(u32);
            return null;
        }

        return prev_res;
    }

    pub inline fn advanceTo(self: *PostingsIteratorV2, target_id: u32) !?Result {
        if (self.consumed) {
            return null;
        }

        var idx = self.current_doc_idx % BLOCK_SIZE;
        if (self.uncompressed_doc_ids_buffer[idx] >= target_id) {
            return .{
                .doc_id    = self.uncompressed_doc_ids_buffer[idx],
                .term_freq = self.uncompressed_tfs_buffer[idx],
                .term_pos  = undefined,
            };
        }

        const partial_block = self.partial();
        if (!self.on_partial_block) {
            const num_full_blocks = partial_block.num_full_blocks;

            std.debug.assert(num_full_blocks > 0);

            var block_idx = self.current_doc_idx >> (comptime std.math.log2(BLOCK_SIZE));
            var prev_next_block_idx = self.fb_meta.next_block_idx;

            var skipped_block = false;
            while (block_idx < num_full_blocks) {
                // std.debug.print(
                    // "Current idx: {d} | Block idx: {d}/{d} | Target id: {d} | FB META: {any}\n",
                    // .{self.current_doc_idx, block_idx, num_full_blocks, target_id, self.fb_meta},
                // );

                if (target_id > self.fb_meta.max_doc_id) {
                    idx = 0;
                    block_idx += 1;
                    self.current_doc_idx = std.mem.alignForward(
                        usize,
                        self.current_doc_idx + 1,
                        BLOCK_SIZE,
                    );
                    skipped_block = true;
                    if (block_idx < num_full_blocks - 1) {
                        prev_next_block_idx = self.fb_meta.next_block_idx;
                        self.fb_meta = try PostingsBlockFullV2.getNextMeta(
                            self.postings_list.full_buffer,
                            self.fb_meta.next_block_idx,
                            );
                    }
                    continue;
                }

                // Doc id in full block if here.
                if (skipped_block) {
                    std.debug.assert(prev_next_block_idx != std.math.maxInt(u32));
                    self.fb_meta = try PostingsBlockFullV2.decompressToBuffers(
                        &self.uncompressed_doc_ids_buffer,
                        &self.uncompressed_tfs_buffer,
                        self.postings_list.full_buffer,
                        prev_next_block_idx,
                        );
                }

                if (linearLowerBoundSIMD(
                    &self.uncompressed_doc_ids_buffer,
                    idx,
                    target_id,
                )) |matched_idx| {

                    self.current_doc_idx = std.mem.alignBackward(
                        usize,
                        self.current_doc_idx,
                        BLOCK_SIZE,
                    ) + matched_idx;

                    self.current_block_max_score = scoreBM25Fast(
                        self.fb_meta.max_tf,
                        self.fb_meta.max_doc_size,
                        self.A,
                        self.C2,
                    );
                    return .{
                        .doc_id    = self.uncompressed_doc_ids_buffer[matched_idx],
                        .term_freq = self.uncompressed_tfs_buffer[matched_idx],
                        .term_pos  = undefined,
                    };
                }

                unreachable;
            }

            partial_block.decompressToBuffers(
                &self.uncompressed_doc_ids_buffer,
                &self.uncompressed_tfs_buffer,
                );
            self.on_partial_block = true;
        }

        const match_idx = linearLowerBound(
            self.uncompressed_doc_ids_buffer[idx..partial_block.num_docs],
            target_id,
        );
        self.current_doc_idx += match_idx;

        if (self.current_doc_idx >= self.posting_len) {
            self.consumed = true;
            self.current_doc_idx = std.math.maxInt(u32);
            return null;
        }

        self.current_block_max_score = scoreBM25Fast(
                partial_block.max_tf,
                partial_block.max_doc_size,
                self.A,
                self.C2,
            );

        idx = self.current_doc_idx % BLOCK_SIZE;
        return .{
            .doc_id    = self.uncompressed_doc_ids_buffer[idx],
            .term_freq = self.uncompressed_tfs_buffer[idx],
            .term_pos  = undefined,
        };
    }
};

pub const QueryResult = packed struct(u64) {
    doc_id: u32,
    partition_idx: u32,
};

pub const SHM = std.HashMap([]const u8, []const u8, CaseInsensitiveWyhashContext, 80);

const IndexContext = struct {
    string_bytes: *const std.ArrayListUnmanaged(u8),

    pub fn eql(_: IndexContext, a: u32, b: u32) bool {
        return a == b;
    }

    pub fn hash(self: IndexContext, x: u32) u64 {
        const x_slice = std.mem.span(
            @as([*:0]u8, @ptrCast(&self.string_bytes.items[x]))
            );
        return CaseInsensitiveWyhash.hash(42, x_slice);
    }
};

const SliceAdapter = struct {
    string_bytes: *const std.ArrayListUnmanaged(u8),

    pub fn eql(self: SliceAdapter, a_slice: []u8, b: u32) bool {
        const b_slice = std.mem.span(
            @as([*:0]u8, @ptrCast(&self.string_bytes.items[b])),
            );

        if (a_slice.len != b_slice.len) return false;
        for (0..a_slice.len) |i| {
            if (std.ascii.toLower(a_slice[i]) != std.ascii.toLower(b_slice[i])) {
                return false;
            }
        }
        return true;
    }

    pub fn hash(_: SliceAdapter, adapted_key: []u8) u64 {
        return CaseInsensitiveWyhash.hash(42, adapted_key);
    }
};

const Vocab = struct {
    string_bytes: std.ArrayListUnmanaged(u8),
    map: std.hash_map.HashMapUnmanaged(u32, u32, IndexContext, 80),

    pub fn init() Vocab {
        return Vocab{
            .string_bytes = .{},
            .map = .{},
        };
    }

    pub fn deinit(self: *Vocab, allocator: std.mem.Allocator) void {
        self.string_bytes.deinit(allocator);
        self.map.deinit(allocator);
    }

    pub inline fn getCtx(self: *Vocab) IndexContext {
        return IndexContext{ .string_bytes = &self.string_bytes };
    }

    pub inline fn getAdapter(self: *Vocab) SliceAdapter {
        return SliceAdapter{ .string_bytes = &self.string_bytes };
    }

    pub inline fn get(self: *Vocab, key: []u8, adapter: SliceAdapter) ?u32 {
        return self.map.getAdapted(key, adapter);
    }
};

pub const BLOCK_SIZE: usize = 64;
pub const NUM_BLOCKS: usize = @divExact(BLOCK_SIZE, 16);

inline fn putBits(buf: [*]u8, bit_pos: usize, value: u32, lo_bits: u8) void {
    var v: u32 = value;
    var written: u8 = 0;

    while (written < lo_bits) {
        const byte_idx = (bit_pos + written) >> 3;
        const bit_idx  = (bit_pos + written) & 7;
        const chunk: u5 = @truncate(@min(8 - bit_idx, lo_bits - written));

        const mask: u32 = (@as(u32, 1) << chunk) - 1;
        const bits: u8  = @truncate((v & mask) << @truncate(bit_idx));

        buf[byte_idx] |= bits;
        v >>= chunk;
        written += chunk;
    }
}

inline fn getBits32(
    buf: [*]const u8,
    bit_pos: usize,
    width: usize,
) u32 {
    const byte_off = bit_pos >> 3;
    const lo_bits  = bit_pos & 7;

    // This will be aligned, but the compiler can't figure it out.
    const word_ptr: *align(1) u64 = @constCast(
        @alignCast(@ptrCast(buf + byte_off))
    );
    const word: u64 = word_ptr.*;

    const value = (word >> @truncate(lo_bits)) & ((@as(u64, 1) << @truncate(width)) - 1);
    return @as(u32, @truncate(value));
}

pub const DeltaBitpackedBlock = packed struct (u64) {
    min_val: u32,
    bit_size: u8,
    _: u24,


    pub fn build(
        buffer: []u8,
        buffer_idx: *u64,
        scratch_arr: *align(64) [BLOCK_SIZE]u32,
        sorted_vals: *align(64) [BLOCK_SIZE]u32,
        ) !DeltaBitpackedBlock {
        var dbp = DeltaBitpackedBlock{
            .min_val = sorted_vals[0],
            .bit_size = undefined,
            ._ = undefined,
        };
        var block_maxes: @Vector(NUM_BLOCKS, u32) = @splat(0);

        const sv_vec: [*]@Vector(16, u32) = @ptrCast(sorted_vals);
        var scratch_vec: [*]@Vector(16, u32) = @ptrCast(scratch_arr);

        inline for (0..NUM_BLOCKS) |block_idx| {
            const start_idx = comptime block_idx * 16;
            const shift_in_idx = comptime if (block_idx == NUM_BLOCKS - 1) 
                BLOCK_SIZE - 1
             else 
                start_idx + 16;
            

            scratch_vec[block_idx] = std.simd.shiftElementsLeft(
                sv_vec[block_idx],
                1,
                sorted_vals[shift_in_idx],
            ) - sv_vec[block_idx];
            block_maxes[block_idx] = @reduce(.Max, scratch_vec[block_idx]);
        }

        const max_gap: u32 = @reduce(.Max, block_maxes);
        dbp.bit_size = 32 - @clz(max_gap);
        const buffer_size = ((BLOCK_SIZE * dbp.bit_size) + 7) >> 3;

        const struct_size = comptime @sizeOf(DeltaBitpackedBlock);
        @memcpy(
            buffer[buffer_idx.*..][0..struct_size], 
            std.mem.asBytes(&dbp),
            );
        buffer_idx.* += struct_size;

        @memset(buffer[buffer_idx.*..][0..buffer_size], 0);

        var bit_pos: usize = buffer_idx.* << 3;
        inline for (0..BLOCK_SIZE) |idx| {
            const gap: u32  = scratch_arr[idx];

            putBits(buffer.ptr, bit_pos, gap, dbp.bit_size);
            bit_pos += dbp.bit_size;
        }

        scratch_arr[BLOCK_SIZE - 1] = sorted_vals[BLOCK_SIZE - 1];
        buffer_idx.* = try std.math.divCeil(
            u64,
            bit_pos,
            8,
        );
        return dbp;
    }
};

pub const BitpackedBlock = packed struct (u64) {
    max_val: u32,
    bit_size: u8,
    _: u24,

    pub fn build(
        buffer: []u8,
        buffer_idx: *u64,
        vals: *align(64) [BLOCK_SIZE]u16,
        ) !BitpackedBlock {
        var bp = BitpackedBlock{
            .max_val = undefined,
            .bit_size = undefined,
            ._ = undefined,
        };
        var block_maxes: @Vector(NUM_BLOCKS, u16) = @splat(0);

        const sv_vec: [*]@Vector(16, u16) = @ptrCast(vals);
        inline for (0..NUM_BLOCKS) |block_idx| {
            block_maxes[block_idx] = @reduce(.Max, sv_vec[block_idx]);
        }

        bp.max_val = @reduce(.Max, block_maxes);
        bp.bit_size = 16 - @clz(@as(u16, @truncate(bp.max_val)));
        const buffer_size = ((BLOCK_SIZE * bp.bit_size) + 7) >> 3;

        const struct_size = comptime @sizeOf(BitpackedBlock);
        @memcpy(
            buffer[buffer_idx.*..][0..struct_size], 
            std.mem.asBytes(&bp),
            );
        buffer_idx.* += struct_size;

        @memset(buffer[buffer_idx.*..][0..buffer_size], 0);

        var bit_pos: usize = buffer_idx.* << 3;
        inline for (0..BLOCK_SIZE) |idx| {
            const val: u16  = vals[idx];

            putBits(buffer.ptr, bit_pos, val, bp.bit_size);
            bit_pos += bp.bit_size;
        }
        buffer_idx.* = try std.math.divCeil(
            u64,
            bit_pos,
            8,
        );

        return bp;
    }

};

pub const PostingsBlockFullV2 = packed struct {
    max_doc_id: u32,
    max_tf: u16,
    max_doc_size: u16,

    doc_id_buf_len: u16,
    tfs_buf_len: u16,
    tp_buf_len: u32,

    next_block_idx: u32,

    // doc_ids: DeltaBitpackedBlock,
    // tfs:     BitpackedBlock,
    // term_positions: [*]align(POST_ALIGNMENT)u8,
    // TODO: Make this many item pointer a u32 index into a large buffer.

    // This will be an inline buffer.
    // It will be up to functions to interpret the raw buffer as the proper
    // struct values.
    //  --------------------- 
    // | docs_ids | tfs | tp |
    //  --------------------- 

    pub inline fn getNextMeta(
        full_block_buffer: []u8,
        start_idx: u64,
    ) !PostingsBlockFullV2 {
        const pbf_size = comptime @sizeOf(PostingsBlockFullV2);
        const meta = std.mem.bytesAsValue(
            PostingsBlockFullV2,
            full_block_buffer[start_idx..][0..pbf_size],
            ).*;
        return meta;
    }

    pub inline fn decompressToBuffers(
        doc_ids: *[BLOCK_SIZE]u32,
        tfs: *[BLOCK_SIZE]u16,
        full_block_buffer: []u8,
        start_idx: u64,
    ) !PostingsBlockFullV2 {
        const pbf_size = comptime @sizeOf(PostingsBlockFullV2);
        const meta = std.mem.bytesAsValue(
            PostingsBlockFullV2,
            full_block_buffer[start_idx..][0..pbf_size],
            ).*;
        var buf = full_block_buffer[start_idx + pbf_size..];

        const dbp_size = comptime @sizeOf(DeltaBitpackedBlock);
        const doc_id_block = std.mem.bytesAsValue(
            DeltaBitpackedBlock,
            buf[0..dbp_size],
            ).*;
        buf = buf[dbp_size..];

        const d_bits: usize = doc_id_block.bit_size;
        var bit_pos: usize = 0;

        var prev: u32 = doc_id_block.min_val;
        doc_ids[0] = prev;

        if (d_bits == 0) {
            @branchHint(.cold);
            for (1..BLOCK_SIZE) |i| doc_ids[i] = prev;
        } else {
            for (1..BLOCK_SIZE) |i| {

                const gap: u32 = getBits32(
                    buf.ptr,
                    bit_pos, 
                    d_bits,
                    );
                bit_pos += d_bits;

                const cur: u32 = prev + gap;
                doc_ids[i] = cur;
                prev = cur;
            }
        }

        buf = buf[meta.doc_id_buf_len - dbp_size..];

        const bp_size = comptime @sizeOf(DeltaBitpackedBlock);
        const tf_block = std.mem.bytesAsValue(
            BitpackedBlock,
            buf[0..bp_size],
            ).*;
        buf = buf[bp_size..];

        const tf_bits: usize = tf_block.bit_size;
        bit_pos = 0;

        if (tf_bits == 0) {
            @branchHint(.cold);
            @memset(tfs, 1);
        } else {
            inline for (0..BLOCK_SIZE) |i| {
                const v = getBits32(
                    buf.ptr,
                    bit_pos, 
                    tf_bits,
                    );
                bit_pos += tf_bits;
                tfs[i] = @truncate(v);
            }
        }

        // TODO: TP Block

        return meta;
    }
};

// pub const PostingsBlockFull = struct {
    // doc_ids: DeltaBitpackedBlock,
    // tfs:     BitpackedBlock,
    // // term_positions: [*]align(POST_ALIGNMENT)u8,
    // // TODO: Make this many item pointer a u32 index into a large buffer.
    // // term_positions: [*]u8,
    // term_pos_idx_ptr: u32,
    // tp_buf_len: u32,
    // max_tf: u16,
    // max_doc_size: u16,
    // max_doc_id: u32,
// 
    // // TODO: Store `impacts` (scores) as u16.
    // //       Keep small (~4) exception array of highest impact docs, [impact, doc_id]
// 
    // pub inline fn decompressToBuffers(
        // self: *const PostingsBlockFull,
        // doc_ids: *[BLOCK_SIZE]u32,
        // tfs: *[BLOCK_SIZE]u16,
    // ) !void {
        // const d_bits: usize = self.doc_ids.bit_size;
        // var bit_pos: usize = 0;
// 
        // var prev: u32 = self.doc_ids.min_val;
        // doc_ids[0] = prev;
// 
        // if (d_bits == 0) {
            // @branchHint(.cold);
            // for (1..BLOCK_SIZE) |i| doc_ids[i] = prev;
        // } else {
            // for (1..BLOCK_SIZE) |i| {
                // // @setEvalBranchQuota(1600);
// 
                // const gap: u32 = getBits32(self.doc_ids.buffer, bit_pos, d_bits);
                // bit_pos += d_bits;
// 
                // const cur: u32 = prev + gap;
                // doc_ids[i] = cur;
                // prev = cur;
            // }
        // }
// 
        // const tf_bits: usize = self.tfs.bit_size;
        // bit_pos = 0;
// 
        // if (tf_bits == 0) {
            // @branchHint(.cold);
            // @memset(tfs, 1);
        // } else {
            // inline for (0..BLOCK_SIZE) |i| {
                // const v = getBits32(self.tfs.buffer, bit_pos, tf_bits);
                // bit_pos += tf_bits;
                // tfs[i] = @truncate(v);
            // }
        // }
    // }
// };


const DeltaVByteBlock = struct {
    // TODO: Consider making these page aligned.
    //       Reconsider alignment and allocations in these structs
    //       to try to minimize faults and prevent fragmentation.
    //
    //       Also consider using raw pointer with size as a smaller
    //       integer type.
    buffer: std.ArrayListAlignedUnmanaged(u8, POST_ALIGNMENT),

    pub fn init() DeltaVByteBlock {
        return DeltaVByteBlock{
            .buffer = std.ArrayListAlignedUnmanaged(u8, POST_ALIGNMENT){},
        };
    }

    pub fn build(
        allocator: std.mem.Allocator,
        sorted_vals: []u32,
        ) !DeltaVByteBlock {

        var bytes_needed: usize = 0;
        for (sorted_vals) |val| {
            const delta: u32 = if (val == 0) 0 else val - sorted_vals[0];
            bytes_needed += pq.getVbyteSizeTable(
                @TypeOf(delta),
                delta,
                );
        }
        const dvb = DeltaVByteBlock{
            .buffer = try std.ArrayListAlignedUnmanaged(
                u8, 
                POST_ALIGNMENT,
                ).initCapacity(
                allocator,
                bytes_needed,
                ),
        };
        var buf_idx: usize = 0;
        for (sorted_vals) |val| {
            const delta = if (val == 0) 0 else val - sorted_vals[0];
            pq.encodeVbyteTable(
                @TypeOf(delta),
                dvb.buffer.items.ptr,
                &buf_idx,
                delta,
            );
        }

        return dvb;
    }

    pub inline fn add(
        self: *DeltaVByteBlock,
        allocator: std.mem.Allocator,
        value: u32,
        prev_val: u32,
    ) !void {
        const bytes_needed = pq.getVbyteSizeTable(
            @TypeOf(value),
            value - prev_val,
            );
        var init_ptr = self.buffer.items.len;
        try self.buffer.resize(
            allocator,
            init_ptr + bytes_needed
        );
        pq.encodeVbyteTable(
            @TypeOf(value),
            self.buffer.items.ptr,
            &init_ptr,
            value - prev_val,
        );
    }

    pub fn buildIntoDeltaBitpacked(
        self: *DeltaVByteBlock,
        // allocator: std.mem.Allocator,
        buffer: []u8,
        buffer_idx: *u64,
        scratch_arr: *align(64) [BLOCK_SIZE]u32,
        ) !DeltaBitpackedBlock {
        // Optimize later. For now, just convert to u32 buf, then call above func.

        // TODO: Thread local static arrays.
        var tmp_arr: [BLOCK_SIZE]u32 align(64) = undefined;

        var prev_val: u32 = 0;
        var byte_ptr: usize = 0;
        for (0..BLOCK_SIZE) |idx| {
            tmp_arr[idx] = @as(u32, @truncate(pq.decodeVbyte(
                self.buffer.items.ptr,
                &byte_ptr,
            ))) + prev_val;
            prev_val = tmp_arr[idx];
        }

        return try DeltaBitpackedBlock.build(
            // allocator,
            buffer,
            buffer_idx,
            scratch_arr,
            &tmp_arr,
        );
    }

    pub inline fn clear(self: *DeltaVByteBlock) void {
        self.buffer.clearRetainingCapacity();
    }
};

const VByteBlock = struct {
    buffer: std.ArrayListAlignedUnmanaged(
                u8, 
                POST_ALIGNMENT,
                ),

    pub fn init() VByteBlock {
        return VByteBlock{
            .buffer = std.ArrayListAlignedUnmanaged(
                u8, 
                POST_ALIGNMENT,
                ){},
        };
    }

    pub fn build(
        allocator: std.mem.Allocator,
        vals: [BLOCK_SIZE]u32,
        ) !VByteBlock {
        var bytes_needed: usize = 0;
        for (vals) |val| {
            bytes_needed += pq.getVbyteSizeTable(@TypeOf(val), val);
        }
        const vb = VByteBlock{
            .buffer = try std.ArrayListAlignedUnmanaged(
                u8, 
                POST_ALIGNMENT,
                ).initCapacity(
                allocator,
                bytes_needed,
                ),
        };

        var buf_idx: usize = 0;
        for (vals) |val| {
            pq.encodeVbyteTable(
                @TypeOf(val),
                vb.buffer.items.ptr,
                &buf_idx,
                val,
            );
        }

        return vb;
    }

    pub inline fn add(
        self: *VByteBlock,
        allocator: std.mem.Allocator,
        value: u32,
    ) !void {
        const bytes_needed = pq.getVbyteSizeTable(@TypeOf(value), value);
        const new_slice = try self.buffer.addManyAsSlice(
            allocator,
            bytes_needed,
        );
        var idx: usize = 0;
        pq.encodeVbyteTable(
            @TypeOf(value),
            new_slice.ptr,
            &idx,
            value,
        );
    }

    pub fn buildIntoBitpacked(
        self: *VByteBlock,
        // allocator: std.mem.Allocator,
        buffer: []u8,
        buffer_idx: *u64,
        ) !BitpackedBlock {
        // Optimize later. For now, just convert to u32 buf, then call above func.

        // TODO: Thread local static arrays.
        var tmp_arr: [BLOCK_SIZE]u16 align(64) = undefined;

        // TODO: Fix this.
        self.buffer.items[0] &= 0b01111111;

        var byte_ptr: usize = 0;
        for (0..BLOCK_SIZE) |idx| {
            // std.debug.print("IDX: {d}\n", .{idx});
            // std.debug.print("POS: {d}/{d}\n", .{byte_ptr, self.buffer.items.len});
            tmp_arr[idx] = @truncate(pq.decodeVbyte(
                self.buffer.items.ptr,
                &byte_ptr,
            ));
            // std.debug.print("VALUE: {d}\n", .{tmp_arr[idx]});
            // std.debug.print("IDX: {d}\n\n", .{idx});
        }

        return try BitpackedBlock.build(buffer, buffer_idx, &tmp_arr);
    }

    pub inline fn clear(self: *VByteBlock) void {
        self.buffer.clearRetainingCapacity();
    }
};

pub const DocIDPostingsBlockPartialChunk = packed struct (u32) {
    // doc_ids: DeltaVByteBlock,
    // tfs:     VByteBlock,
    // term_positions: std.ArrayListAlignedUnmanaged(u8, POST_ALIGNMENT),
    // tp_indexing: std.ArrayListAlignedUnmanaged(u16, POST_ALIGNMENT),
};

pub const PostingsFreeLists = struct {
    fl_16: std.ArrayListUnmanaged(u32),
    fl_32: std.ArrayListUnmanaged(u32),
    fl_64: std.ArrayListUnmanaged(u32),
    fl_128: std.ArrayListUnmanaged(u32),
    fl_256: std.ArrayListUnmanaged(u32),
    fl_512: std.ArrayListUnmanaged(u32),
    fl_1024: std.ArrayListUnmanaged(u32),

    pub fn init() PostingsFreeLists {
        return PostingsFreeLists{
            .fl_16 = std.ArrayListUnmanaged(u32){},
            .fl_32 = std.ArrayListUnmanaged(u32){},
            .fl_64 = std.ArrayListUnmanaged(u32){},
            .fl_128 = std.ArrayListUnmanaged(u32){},
            .fl_256 = std.ArrayListUnmanaged(u32){},
            .fl_512 = std.ArrayListUnmanaged(u32){},
            .fl_1024 = std.ArrayListUnmanaged(u32){},
        };
    }
    
    pub fn deinit(self: *PostingsFreeLists, allocator: std.mem.Allocator) void {
        self.fl_16.deinit(allocator);
        self.fl_32.deinit(allocator);
        self.fl_64.deinit(allocator);
        self.fl_128.deinit(allocator);
        self.fl_256.deinit(allocator);
        self.fl_512.deinit(allocator);
        self.fl_1024.deinit(allocator);
    }

    pub fn add(
        self: *PostingsFreeLists, 
        allocator: std.mem.Allocator,
        start_pos: usize,
        buf_size: usize,
        ) !?u32 {
        if (buf_size > 1024) {
            // TODO: Determine which buf_type is best to add to.
            //       For now just do largest 1024.
            const num_new_bufs = buf_size >> 10;

            var new_pos = start_pos;
            for (0..num_new_bufs) |_| {
                try self.fl_1024.append(allocator, @truncate(new_pos));
                new_pos += 1024;
            }
            return null;
        }
        switch (buf_size) {
            16 => {
                try self.fl_16.append(allocator, @truncate(start_pos));
                return self.fl_32.pop();
            },
            32 => {
                try self.fl_32.append(allocator, @truncate(start_pos));
                return self.fl_64.pop();
            },
            64 => {
                try self.fl_64.append(allocator, @truncate(start_pos));
                return self.fl_128.pop();
            },
            128 => {
                try self.fl_128.append(allocator, @truncate(start_pos));
                return self.fl_256.pop();
            },
            256 => {
                try self.fl_256.append(allocator, @truncate(start_pos));
                return self.fl_512.pop();
            },
            512 => {
                try self.fl_512.append(allocator, @truncate(start_pos));
                return self.fl_1024.pop();
            },
            1024 => {
                try self.fl_1024.append(allocator, @truncate(start_pos));
                return null;
            },
            else => @panic(
                "Buf size not supported. Please choose a power of two greater than or equal to 16."
                ),
        }
    }
};

pub const PostingsListIndexing = struct {
    // Size: (5 * 4 * num_terms) + epsilon + (3 * 256MiB)

    d_chunk_start_idxs: std.ArrayListUnmanaged(u32),
    d_buffer: []u8,
    d_bump_idx: u32,
    d_free_lists: PostingsFreeLists,
    prev_doc_ids: std.ArrayListUnmanaged(u32),

    tf_chunk_start_idxs: std.ArrayListUnmanaged(u32),
    tf_buffer: []u8,
    tf_bump_idx: u32,
    tf_free_lists: PostingsFreeLists,

    tp_chunk_start_idxs: std.ArrayListUnmanaged(u32),
    tp_buffer: []u8,
    tp_bump_idx: u32,
    tp_free_lists: PostingsFreeLists,

    doc_freqs: std.ArrayListUnmanaged(u32),

    pub fn init(allocator: std.mem.Allocator) !PostingsListIndexing {
        return PostingsListIndexing{
            .d_chunk_start_idxs = std.ArrayListUnmanaged(u32){},
            .d_buffer = try allocator.alloc(u8, 1 << 28), // 256 MiB
            .d_bump_idx = 0,
            .d_free_lists = PostingsFreeLists.init(),
            .prev_doc_ids = std.ArrayListUnmanaged(u32){},

            .tf_chunk_start_idxs = std.ArrayListUnmanaged(u32){},
            .tf_buffer = try allocator.alloc(u8, 1 << 28), // 256 MiB
            .tf_bump_idx = 0,
            .tf_free_lists = PostingsFreeLists.init(),

            .tp_chunk_start_idxs = std.ArrayListUnmanaged(u32){},
            .tp_buffer = try allocator.alloc(u8, 1 << 30), // 1 GiB
            .tp_bump_idx = 0,
            .tp_free_lists = PostingsFreeLists.init(),

            .doc_freqs = std.ArrayListUnmanaged(u32){},
        };
    }

    pub fn deinit(self: *PostingsListIndexing, allocator: std.mem.Allocator) void {
        self.d_chunk_start_idxs.deinit(allocator);
        self.tf_chunk_start_idxs.deinit(allocator);
        self.tp_chunk_start_idxs.deinit(allocator);

        self.d_free_lists.deinit(allocator);
        self.tf_free_lists.deinit(allocator);
        self.tp_free_lists.deinit(allocator);

        allocator.free(self.d_buffer);
        allocator.free(self.tf_buffer);
        allocator.free(self.tp_buffer);

        self.doc_freqs.deinit(allocator);
        self.prev_doc_ids.deinit(allocator);
    }

    pub inline fn appendToBuf(
        comptime T: type,
        start_pos: *u32,
        bump_idx: *u32,
        global_buf: []u8,
        freelists: *PostingsFreeLists,
        new_val: T,
    ) void {
        // try self.prev_doc_ids.append(gpa.allocator(), doc_id);

        var buf: []u8 = undefined;
        if (freelists.fl_16.pop()) |free_idx_16| {
            start_pos.* = free_idx_16;

            buf = global_buf[free_idx_16..][0..16];
        } else {
            start_pos.* = bump_idx.*;

            buf = global_buf[bump_idx.*..][0..16];
            bump_idx.* += 16;
        }
        buf[0] = comptime std.math.log2(16);

        var idx: usize = 1;
        pq.encodeVbyteTable(
            @TypeOf(new_val),
            buf.ptr,
            &idx,
            new_val,
        );
        buf[15] = @truncate(idx);
    }

    pub inline fn addToBuf(
        comptime T: type,
        gpa: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        start_pos: *u32,
        bump_idx: *u32,
        global_buf: []u8,
        freelists: *PostingsFreeLists,
        new_val: T,
        prev_val: ?*T,
    ) !void {
        var current_pos: usize = undefined;

        var buf = global_buf[start_pos.*..];
        var buf_size = @as(u32, 1) << @truncate(buf[0]);

        // if (buf_size <= (comptime 1 << 8)) {
            // current_pos = buf[buf_size-1..][0];
        // } else if (buf_size <= (comptime 1 << 16)) {
            // current_pos = std.mem.readInt(
                // u16,
                // buf[buf_size-3..][0..2],
                // std.builtin.Endian.little,
            // );
        // } else {
            // current_pos = std.mem.readInt(
                // u32,
                // buf[buf_size-5..][0..4],
                // std.builtin.Endian.little,
            // );
        // }
        current_pos = std.mem.readInt(
            u32,
            buf[buf_size-5..][0..4],
            std.builtin.Endian.little,
        );

        const footer_size: u32 = 4;

        const space_remaining = buf_size - footer_size - current_pos;
        var space_needed: usize = undefined;
        var write_val: T = undefined;
        if (prev_val) |pv| {
            space_needed = pq.getVbyteSizeTable(
                T,
                new_val - pv.*,
            );
            write_val = new_val - pv.*;
            pv.* = new_val;
        } else {
            space_needed = pq.getVbyteSizeTable(
                T,
                new_val,
            );
            write_val = new_val;
        }

        if (space_needed > space_remaining) {
            const _new_start_idx = try freelists.add(
                gpa.allocator(), 
                @intCast(start_pos.*),
                @intCast(buf_size),
                );

            const old_start_idx = start_pos.*;
            if (_new_start_idx) |new_start_idx| {
                start_pos.* = new_start_idx;
            } else {
                start_pos.* = bump_idx.*;
                bump_idx.* += buf_size << 1;
            }

            @memcpy(
                global_buf[start_pos.*..][0..buf_size],
                global_buf[old_start_idx..][0..buf_size],
            );

            buf_size <<= 1;
            buf = global_buf[start_pos.*..][0..buf_size];
        }
        buf[0] = @truncate(std.math.log2(buf_size));

        var dummy_idx = @as(usize, @intCast(current_pos));
        pq.encodeVbyteTable(
            @TypeOf(write_val),
            buf.ptr,
            &dummy_idx,
            write_val,
        );
        current_pos = @truncate(dummy_idx);

        // if (buf_size <= (comptime 1 << 8)) {
            // buf[buf_size-1..][0] = @truncate(current_pos);
        // } else if (buf_size <= (comptime 1 << 16)) {
             // std.mem.writeInt(
                // u16,
                // buf[buf_size-3..][0..2],
                // @as(u16, @truncate(current_pos)),
                // std.builtin.Endian.little,
            // );
        // } else {
            // std.mem.writeInt(
                // u32,
                // buf[buf_size-5..][0..4],
                // @as(u32, @truncate(current_pos)),
                // std.builtin.Endian.little,
            // );
        // }
        std.mem.writeInt(
            u32,
            buf[buf_size-5..][0..4],
            @as(u32, @truncate(current_pos)),
            std.builtin.Endian.little,
        );
    }

    pub inline fn incBuf(
        gpa: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        start_pos: *u32,
        bump_idx: *u32,
        global_buf: []u8,
        freelists: *PostingsFreeLists,
    ) !void {
        var current_pos: usize = undefined;

        var buf = global_buf[start_pos.*..];
        var buf_size = @as(u32, 1) << @truncate(buf[0]);
        // if (buf_size <= (comptime 1 << 8)) {
            // current_pos = buf[buf_size-1..][0];
        // } else if (buf_size <= (comptime 1 << 16)) {
            // current_pos = std.mem.readInt(
                // u16,
                // buf[buf_size-3..][0..2],
                // std.builtin.Endian.little,
            // );
        // } else {
            // current_pos = std.mem.readInt(
                // u32,
                // buf[buf_size-5..][0..4],
                // std.builtin.Endian.little,
            // );
        // }
        current_pos = std.mem.readInt(
            u32,
            buf[buf_size-5..][0..4],
            std.builtin.Endian.little,
        );

        const prev_size = pq.getPrevVByteSize(
            buf.ptr,
            @intCast(current_pos),
        );
        const prev_idx = std.math.sub(
            u32, 
            @truncate(current_pos), 
            @truncate(prev_size)
            ) catch 1;

        var dummy_idx: usize = prev_idx;
        const prev_value = pq.decodeVbyte(
            buf.ptr,
            &dummy_idx,
        );
        current_pos = prev_idx;

        const footer_size: u32 = 4;

        const space_remaining = buf_size - footer_size - current_pos;
        var space_needed: usize = undefined;
        space_needed = pq.getVbyteSizeTable(
            @TypeOf(prev_value),
            prev_value + 1, 
        );

        if (space_needed > space_remaining) {
            const _new_start_idx = try freelists.add(
                gpa.allocator(), 
                @intCast(start_pos.*),
                @intCast(buf_size),
                );

            const old_start_idx = start_pos.*;
            if (_new_start_idx) |new_start_idx| {
                start_pos.* = new_start_idx;
            } else {
                start_pos.* = bump_idx.*;
                bump_idx.* += buf_size << 1;
            }

            @memcpy(
                global_buf[start_pos.*..][0..buf_size],
                global_buf[old_start_idx..][0..buf_size],
            );

            buf_size <<= 1;
            buf = global_buf[start_pos.*..][0..buf_size];
        }
        buf[0] = @truncate(std.math.log2(buf_size));

        dummy_idx = prev_idx;
        pq.encodeVbyteTable(
            @TypeOf(prev_value),
            buf.ptr,
            &dummy_idx,
            prev_value + 1,
        );
        current_pos = dummy_idx;

        // if (buf_size <= (comptime 1 << 8)) {
            // buf[buf_size-1..][0] = @truncate(current_pos);
        // } else if (buf_size <= (comptime 1 << 16)) {
             // std.mem.writeInt(
                // u16,
                // buf[buf_size-3..][0..2],
                // @as(u16, @truncate(current_pos)),
                // std.builtin.Endian.little,
            // );
        // } else {
            // std.mem.writeInt(
                // u32,
                // buf[buf_size-5..][0..4],
                // @as(u32, @truncate(current_pos)),
                // std.builtin.Endian.little,
            // );
        // }
        std.mem.writeInt(
            u32,
            buf[buf_size-5..][0..4],
            @as(u32, @truncate(current_pos)),
            std.builtin.Endian.little,
        );
    }

    pub fn add(
        self: *PostingsListIndexing,
        gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        term_id: u32,
        doc_id: u32,
        term_pos: u16,
    ) !void {

        const new_term = term_id == self.d_chunk_start_idxs.items.len;
        if (new_term) {
            // Doc Ids
            try self.prev_doc_ids.append(gpa.allocator(), doc_id);
            PostingsListIndexing.appendToBuf(
                u32,
                try self.d_chunk_start_idxs.addOne(gpa.allocator()),
                &self.d_bump_idx,
                self.d_buffer,
                &self.d_free_lists,
                doc_id,
            );

            // Term Freqs
            PostingsListIndexing.appendToBuf(
                u16,
                try self.tf_chunk_start_idxs.addOne(gpa.allocator()),
                &self.tf_bump_idx,
                self.tf_buffer,
                &self.tf_free_lists,
                1,
            );

            // Term Positions
            PostingsListIndexing.appendToBuf(
                u16,
                try self.tp_chunk_start_idxs.addOne(gpa.allocator()),
                &self.tp_bump_idx,
                self.tp_buffer,
                &self.tp_free_lists,
                term_pos,
            );

            try self.doc_freqs.append(gpa.allocator(), 1);

        } else {

            if (doc_id == self.prev_doc_ids.items[term_id]) {
                try PostingsListIndexing.incBuf(
                    gpa,
                    &self.tf_chunk_start_idxs.items[term_id],
                    &self.tf_bump_idx,
                    self.tf_buffer,
                    &self.tf_free_lists,
                );

                // Term Positions
                try PostingsListIndexing.addToBuf(
                    u16,
                    gpa,
                    &self.tp_chunk_start_idxs.items[term_id],
                    &self.tp_bump_idx,
                    self.tp_buffer,
                    &self.tp_free_lists,
                    term_pos,
                    null,
                );

                return;
            }

            // Doc Ids
            try PostingsListIndexing.addToBuf(
                u32,
                gpa,
                &self.d_chunk_start_idxs.items[term_id],
                &self.d_bump_idx,
                self.d_buffer,
                &self.d_free_lists,
                doc_id,
                &self.prev_doc_ids.items[term_id],
            );

            // Term Freqs
            try PostingsListIndexing.addToBuf(
                u32,
                gpa,
                &self.tf_chunk_start_idxs.items[term_id],
                &self.tf_bump_idx,
                self.tf_buffer,
                &self.tf_free_lists,
                1,
                null,
            );

            // Term Positions
            try PostingsListIndexing.addToBuf(
                u16,
                gpa,
                &self.tp_chunk_start_idxs.items[term_id],
                &self.tp_bump_idx,
                self.tp_buffer,
                &self.tp_free_lists,
                term_pos,
                null,
            );
        }
    }
};

// TODO: PostingsBlockPartialIndexing vs PostingsBlockPartialIndexed
pub const PostingsBlockPartial = struct {
    doc_ids: DeltaVByteBlock,
    tfs:     VByteBlock,
    term_positions: std.ArrayListAlignedUnmanaged(u8, POST_ALIGNMENT),
    tp_indexing: std.ArrayListAlignedUnmanaged(u16, POST_ALIGNMENT),

    prev_doc_id: u32,
    max_tf: u16,
    max_doc_size: u16,
    prev_tf: u16,
    num_full_blocks: u16,
    num_docs: u8,

    prev_fb_idx_ptr: ?*align(1) u32,

    pub fn init() PostingsBlockPartial {
        return PostingsBlockPartial{
            .doc_ids = DeltaVByteBlock.init(),
            .tfs = VByteBlock.init(),
            .term_positions = std.ArrayListAlignedUnmanaged(u8, POST_ALIGNMENT){},
            .tp_indexing = std.ArrayListAlignedUnmanaged(u16, POST_ALIGNMENT){},

            .prev_doc_id = std.math.maxInt(u32),
            .prev_tf = 1,
            .max_tf = 1,
            .max_doc_size = 1,
            .num_full_blocks = 0,
            .num_docs = 0,

            .prev_fb_idx_ptr = null,
        };
    }

    pub inline fn deinit(self: *PostingsBlockPartial, allocator: std.mem.Allocator) void {
        self.tp_indexing.deinit(allocator);
        self.term_positions.deinit(allocator);
        self.doc_ids.buffer.deinit(allocator);
        self.tfs.buffer.deinit(allocator);
    }

    pub fn flush(
        self: *PostingsBlockPartial,
        gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        buffer: []u8,
        buffer_idx: *u64,
        scratch_arr: *align(64) [BLOCK_SIZE]u32,
    ) !void {
        if (self.tp_indexing.items.len > 0) {
            try self.flushTPTermBuf(gpa.allocator());
        }

        try self.partialToFull(buffer, buffer_idx, scratch_arr);

        self.num_docs = 0;
        self.doc_ids.clear();
        self.tfs.clear();
        self.max_tf = 1;
        self.max_doc_size = 1;
        self.prev_doc_id = std.math.maxInt(u32);
        self.prev_tf = 1;
        self.num_full_blocks += 1;
    }

    fn partialToFull(
        self: *PostingsBlockPartial,
        buffer: []u8,
        buffer_idx: *u64,
        scratch_arr: *align(64) [BLOCK_SIZE]u32,
    ) !void {
        std.debug.assert(self.num_docs == BLOCK_SIZE);
        var pbf = PostingsBlockFullV2{
            .max_doc_id = undefined,
            .max_tf = self.max_tf,
            .max_doc_size = self.max_doc_size,

            .doc_id_buf_len = undefined,
            .tfs_buf_len = undefined,
            .tp_buf_len = undefined,

            .next_block_idx = std.math.maxInt(u32),
        };

        const init_idx = buffer_idx.*;

        const pbf_size = comptime @sizeOf(PostingsBlockFullV2);
        buffer_idx.* += pbf_size;

        if (self.prev_fb_idx_ptr) |fb_idx_ptr| {
            fb_idx_ptr.* = @truncate(init_idx);
        }

        var start_idx = buffer_idx.*;
        _ = try self.doc_ids.buildIntoDeltaBitpacked(
                buffer,
                buffer_idx,
                scratch_arr,
            );
        pbf.doc_id_buf_len = @truncate(buffer_idx.* - start_idx);

        start_idx = buffer_idx.*;
        _ = try self.tfs.buildIntoBitpacked(
                buffer,
                buffer_idx,
            );
        pbf.tfs_buf_len = @truncate(buffer_idx.* - start_idx);

        @memcpy(
            buffer[buffer_idx.*..][0..self.term_positions.items.len],
            self.term_positions.items,
        );
        buffer_idx.* += self.term_positions.items.len;
        pbf.tp_buf_len = @truncate(self.term_positions.items.len);

        pbf.max_doc_id = scratch_arr[BLOCK_SIZE - 1];

        self.term_positions.clearRetainingCapacity();

        @memcpy(
            buffer[init_idx..][0..pbf_size],
            std.mem.asBytes(&pbf),
        );
        self.prev_fb_idx_ptr = @ptrCast(
            buffer[init_idx + @offsetOf(PostingsBlockFullV2, "next_block_idx")..].ptr
            );

        // std.debug.print("Full buffer size: {d}MiB\n", .{buffer_idx.* >> 20});
    }

    pub inline fn flushTPTermBuf(
        self: *PostingsBlockPartial, 
        allocator: std.mem.Allocator,
        ) !void {
        std.debug.assert(self.tp_indexing.items.len > 0);
        // const start_val: u64 = @intCast(self.tp_indexing.items[0]);

        if (self.tp_indexing.items.len == 1) {
            const bytes_needed = pq.getVbyteSizeTable(
                comptime @TypeOf(self.tp_indexing.items[0]),
                self.tp_indexing.items[0],
                );
            const new_slice = try self.term_positions.addManyAsSlice(
                allocator, 
                bytes_needed,
                );

            var byte_idx: u64 = 0;
            pq.encodeVbyteTable(
                comptime @TypeOf(self.tp_indexing.items[0]),
                new_slice.ptr,
                &byte_idx,
                self.tp_indexing.items[0],
            );
            self.tp_indexing.clearRetainingCapacity();
            return;
        }

        var bit_size:    usize = 0;
        var prev_val:    u16 = self.tp_indexing.items[0];

        var tmp_val: u16 = undefined;
        for (self.tp_indexing.items[1..]) |*val| {
            tmp_val = prev_val;
            prev_val = val.*;
            val.* -= tmp_val;

            bit_size = @max(bit_size, 16 - @clz(val.*));
        }

        const bytes_needed = pq.getVbyteSizeTable(
                comptime @TypeOf(self.tp_indexing.items[0]),
                self.tp_indexing.items[0],
                ) + try std.math.divCeil(
                usize,
                self.tp_indexing.items.len * bit_size,
                8,
                );

        const new_slice = try self.term_positions.addManyAsSlice(
            allocator, 
            bytes_needed,
            );

        var byte_idx: u64 = 0;
        pq.encodeVbyteTable(
            comptime @TypeOf(self.tp_indexing.items[0]),
            new_slice.ptr,
            &byte_idx,
            self.tp_indexing.items[0],
        );
        var bit_idx: u64 = byte_idx << 3;
        for (self.tp_indexing.items[1..]) |val| {
            putBits(new_slice.ptr, bit_idx, val, @truncate(bit_size));
            bit_idx += bit_size;
        }

        self.tp_indexing.clearRetainingCapacity();
    }

    pub inline fn add(
        self: *PostingsBlockPartial,
        gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        doc_id: u32,
        doc_size: u16,
        term_pos: u16,
    ) !void {
        // Needed to increment df in indexing loop.
        if (doc_id != self.prev_doc_id) {
            @branchHint(.likely);

            // TODO: 
            // 1. Swith tf_cntr to tf_max.
            // 2. Implement vbyte addition.
            // 3. On first doc append new element to tfs/doc_ids.
            // 4. Increment tf on subsequent docs.

            try self.tfs.add(gpa.allocator(), 1);

            // try self.doc_ids.add(allocator, doc_id, self.prev_doc_id);
            const delta_base = if (self.num_docs == 0) 0 else self.prev_doc_id;
            try self.doc_ids.add(gpa.allocator(), doc_id, delta_base);

            self.num_docs += 1;
            self.prev_doc_id = doc_id;
            self.prev_tf = 1;

            if (self.num_docs > 1) {
                try self.flushTPTermBuf(gpa.allocator());
            }
            try self.tp_indexing.append(gpa.allocator(), term_pos);

        } else {
            // TODO: Technincally when equal, doc size needs to be considered.
            const new_tf = self.prev_tf + 1;
            const old_size = pq.getVbyteSizeTable(
                @TypeOf(self.prev_tf), 
                self.prev_tf,
                );

            // Remove the old encoded value
            self.tfs.buffer.items.len -= old_size;

            // Append the new encoded value  
            try self.tfs.add(gpa.allocator(), new_tf);

            self.prev_tf     = new_tf;
            self.prev_doc_id = doc_id;
            
            std.debug.assert(term_pos >= self.tp_indexing.getLastOrNull() orelse 0);
            try self.tp_indexing.append(gpa.allocator(), term_pos);

            if (new_tf > self.max_tf) {
                self.max_tf = new_tf;
                self.max_doc_size = doc_size;
            }
        }
    }

    pub inline fn decompressToBuffers(
        self: *const PostingsBlockPartial,
        doc_ids: *[BLOCK_SIZE]u32,
        tfs: *[BLOCK_SIZE]u16,
    ) void {

        var prev_doc_id: u32 = 0;
        var doc_idx: u64 = 0;
        var tf_idx:  u64 = 0;
        for (0..self.num_docs) |idx| {
            doc_ids[idx] = prev_doc_id + @as(u32, @truncate(pq.decodeVbyte(
                self.doc_ids.buffer.items.ptr,
                &doc_idx,
            )));
            tfs[idx] = @truncate(pq.decodeVbyte(
                self.tfs.buffer.items.ptr,
                &tf_idx,
            ));

            prev_doc_id = doc_ids[idx];
        }
    }


    pub fn serialize(
        self: *const PostingsBlockPartial,
        buffer: *std.ArrayListUnmanaged(u8),
        allocator: std.mem.Allocator,
        current_pos: *usize,
        ) !void {
        const max_range = current_pos.* + 16 +
            (self.doc_ids.buffer.items.len + 
             self.tfs.buffer.items.len + 
             self.term_positions.items.len
             );
        if (max_range > buffer.items.len) {
            @branchHint(.unlikely);
            try buffer.resize(allocator, 2 * max_range);
            const range = 2 * max_range - current_pos.*;
            @memset(buffer.items[current_pos.*..][0..range], 0);
        }

        buffer.items[current_pos.*] = self.num_docs;
        current_pos.* += 1;

        pq.encodeVbyteTable(
            @TypeOf(self.max_tf),
            buffer.items.ptr,
            current_pos,
            self.max_tf,
        );
        pq.encodeVbyteTable(
            @TypeOf(self.max_doc_size),
            buffer.items.ptr,
            current_pos,
            self.max_doc_size,
        );

        pq.encodeVbyteTable(
            @TypeOf(self.doc_ids.buffer.items.len),
            buffer.items.ptr,
            current_pos,
            self.doc_ids.buffer.items.len,
        );
        @memcpy(
            buffer.items[current_pos.*..][
                0..self.doc_ids.buffer.items.len
            ],
            self.doc_ids.buffer.items,
        );
        current_pos.* += self.doc_ids.buffer.items.len;

        pq.encodeVbyteTable(
            @TypeOf(self.tfs.buffer.items.len),
            buffer.items.ptr,
            current_pos,
            self.tfs.buffer.items.len,
        );
        @memcpy(
            buffer.items[current_pos.*..][
                0..self.tfs.buffer.items.len
            ],
            self.tfs.buffer.items,
        );
        current_pos.* += self.tfs.buffer.items.len;


        pq.encodeVbyteTable(
            @TypeOf(self.term_positions.items.len),
            buffer.items.ptr,
            current_pos,
            self.term_positions.items.len,
        );
        @memcpy(
            buffer.items[current_pos.*..][
                0..self.term_positions.items.len
            ],
            self.term_positions.items,
        );
        current_pos.* += self.term_positions.items.len;
    }
};

// pub const PostingV3 = struct {
    // full_blocks: std.ArrayListAlignedUnmanaged(
                     // PostingsBlockFull,
                     // POST_ALIGNMENT,
                 // ),
    // partial_block: PostingsBlockPartial,
// 
    // pub inline fn init() PostingV3 {
        // return PostingV3{
            // .full_blocks = std.ArrayListAlignedUnmanaged(
                // PostingsBlockFull,
                // POST_ALIGNMENT,
            // ){},
            // .partial_block = PostingsBlockPartial.init(),
        // };
    // }
   //  
    // pub inline fn deinit(self: *PostingV3, allocator: std.mem.Allocator) void {
        // self.partial_block.tp_indexing.deinit(allocator);
        // self.partial_block.term_positions.deinit(allocator);
        // self.partial_block.doc_ids.buffer.deinit(allocator);
        // self.partial_block.tfs.buffer.deinit(allocator);
        // self.full_blocks.deinit(allocator);
    // }
// 
    // pub inline fn add(
        // self: *PostingV3,
        // gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        // // arena: *std.heap.ArenaAllocator,
        // buffer: []u8,
        // buffer_idx: *u64,
        // doc_id: u32,
        // doc_size: u16,
        // term_pos: u16,
        // scratch_arr: *align(64) [BLOCK_SIZE]u32,
    // ) !void {
        // if (
            // (doc_id != self.partial_block.prev_doc_id)
                // and
            // (self.partial_block.num_docs == BLOCK_SIZE)
        // ) {
            // const new_val = try self.full_blocks.addOne(
                // gpa.allocator(),
                // );
            // new_val.* = try self.partial_block.flush(
                // gpa,
                // // arena, 
                // buffer,
                // buffer_idx,
                // scratch_arr,
                // );
        // }
        // try self.partial_block.add(
            // gpa,
            // // arena,
            // // buffer,
            // // buffer_idx,
            // doc_id,
            // doc_size,
            // term_pos,
        // );
    // }
// 
    // pub fn serialize(
        // self: *PostingV3, 
        // buffer: *std.ArrayListUnmanaged(u8),
        // allocator: std.mem.Allocator,
        // current_pos: *usize,
        // ) !void {
        // // TODO: Add term_positions
// 
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // self.full_blocks.items.len,
        // );
        // for (self.full_blocks.items) |*block| {
            // const d_buf_len  = try std.math.divCeil(
                // usize,
                // block.doc_ids.bit_size * BLOCK_SIZE,
                // 8,
                // );
            // const tf_buf_len = try std.math.divCeil(
                // usize,
                // block.tfs.bit_size * BLOCK_SIZE,
                // 8,
                // );
// 
            // const max_range = current_pos.* + 
                // (d_buf_len + tf_buf_len + @as(usize, (@intCast(block.tp_buf_len)))) + 
                // @sizeOf(u32) * 3 + 
                // @sizeOf(u8) * 2;
// 
            // if (max_range > buffer.items.len) {
                // @branchHint(.unlikely);
                // try buffer.resize(allocator, 2 * max_range);
                // const range = 2 * max_range - current_pos.*;
                // @memset(buffer.items[current_pos.*..][0..range], 0);
            // }
// 
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // @intCast(block.max_doc_id),
            // );
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // @intCast(block.max_tf),
            // );
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // @intCast(block.max_doc_size),
            // );
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // @intCast(block.doc_ids.min_val),
            // );
// 
            // buffer.items[current_pos.*] = block.doc_ids.bit_size;
            // current_pos.* += 1;
// 
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // d_buf_len,
            // );
            // @memcpy(
                // buffer.items[current_pos.*..][0..d_buf_len],
                // block.doc_ids.buffer[0..d_buf_len],
            // );
            // current_pos.* += d_buf_len;
// 
            // buffer.items[current_pos.*] = block.tfs.bit_size;
            // current_pos.* += 1;
// 
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // tf_buf_len,
            // );
            // @memcpy(
                // buffer.items[current_pos.*..][0..tf_buf_len],
                // block.tfs.buffer[0..tf_buf_len],
            // );
            // current_pos.* += tf_buf_len;
// 
            // pq.encodeVbyte(
                // buffer.items.ptr,
                // current_pos,
                // @intCast(block.tp_buf_len),
            // );
            // @memcpy(
                // buffer.items[current_pos.*..][0..block.tp_buf_len],
                // block.term_positions[0..block.tp_buf_len],
            // );
            // current_pos.* += block.tp_buf_len;
        // }
// 
        // const max_range = current_pos.* + 16 +
            // (self.partial_block.doc_ids.buffer.items.len + 
             // self.partial_block.tfs.buffer.items.len + 
             // self.partial_block.term_positions.items.len
             // );
        // if (max_range > buffer.items.len) {
            // @branchHint(.unlikely);
            // try buffer.resize(allocator, 2 * max_range);
            // const range = 2 * max_range - current_pos.*;
            // @memset(buffer.items[current_pos.*..][0..range], 0);
        // }
// 
        // buffer.items[current_pos.*] = self.partial_block.num_docs;
        // current_pos.* += 1;
// 
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // @intCast(self.partial_block.max_tf),
        // );
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // @intCast(self.partial_block.max_doc_size),
        // );
// 
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // self.partial_block.doc_ids.buffer.items.len,
        // );
        // @memcpy(
            // buffer.items[current_pos.*..][
                // 0..self.partial_block.doc_ids.buffer.items.len
            // ],
            // self.partial_block.doc_ids.buffer.items,
        // );
        // current_pos.* += self.partial_block.doc_ids.buffer.items.len;
// 
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // self.partial_block.tfs.buffer.items.len,
        // );
        // @memcpy(
            // buffer.items[current_pos.*..][
                // 0..self.partial_block.tfs.buffer.items.len
            // ],
            // self.partial_block.tfs.buffer.items,
        // );
        // current_pos.* += self.partial_block.tfs.buffer.items.len;
// 
// 
        // pq.encodeVbyte(
            // buffer.items.ptr,
            // current_pos,
            // self.partial_block.term_positions.items.len,
        // );
        // @memcpy(
            // buffer.items[current_pos.*..][
                // 0..self.partial_block.term_positions.items.len
            // ],
            // self.partial_block.term_positions.items,
        // );
        // current_pos.* += self.partial_block.term_positions.items.len;
    // }
// 
    // pub fn deserialize(
        // self: *PostingV3, 
        // allocator: std.mem.Allocator,
        // buffer: []u8,
        // current_pos: *usize,
        // ) !void {
        // const num_full_blocks = pq.decodeVbyte(
            // buffer.ptr,
            // current_pos,
        // );
        // try self.full_blocks.resize(allocator, num_full_blocks);
// 
        // for (0..num_full_blocks) |idx| {
            // var block = &self.full_blocks.items[idx];
            // block.* = PostingsBlockFullV2{
                // .doc_ids = DeltaBitpackedBlock{
                    // .min_val = 0,
                    // .bit_size = 0,
                    // .buffer = undefined,
                // },
                // .tfs = BitpackedBlock{
                    // .max_val = 0,
                    // .bit_size = 0,
                    // .buffer = undefined,
                // },
                // // TODO: Deserialize only the relevant term position(s) if possible.
                // .term_positions = undefined,
                // .tp_buf_len = 0,
                // .max_tf = 1,
                // .max_doc_size = 1,
                // .max_doc_id = 0,
            // };
// 
            // block.max_doc_id      = @truncate(pq.decodeVbyte(buffer.ptr, current_pos));
            // block.max_tf          = @truncate(pq.decodeVbyte(buffer.ptr, current_pos));
            // block.max_doc_size    = @truncate(pq.decodeVbyte(buffer.ptr, current_pos));
            // block.doc_ids.min_val = @truncate(pq.decodeVbyte(buffer.ptr, current_pos));
// 
            // block.doc_ids.bit_size = buffer[current_pos.*];
            // current_pos.* += 1;
// 
            // const doc_id_buf_len = pq.decodeVbyte(buffer.ptr, current_pos);
            // block.doc_ids.buffer = @ptrCast(try allocator.alignedAlloc(
                // u8,
                // POST_ALIGNMENT,
                // doc_id_buf_len,
            // ));
            // @memcpy(
                // block.doc_ids.buffer[0..doc_id_buf_len],
                // buffer[current_pos.*..][0..doc_id_buf_len],
            // );
            // current_pos.* += doc_id_buf_len;
// 
            // block.tfs.bit_size = buffer[current_pos.*];
            // current_pos.* += 1;
// 
            // const tf_buf_len = pq.decodeVbyte(buffer.ptr, current_pos);
            // block.tfs.buffer = @ptrCast(try allocator.alignedAlloc(
                // u8,
                // POST_ALIGNMENT,
                // tf_buf_len,
            // ));
            // @memcpy(
                // block.tfs.buffer[0..tf_buf_len],
                // buffer[current_pos.*..][0..tf_buf_len],
            // );
            // current_pos.* += tf_buf_len;
// 
            // block.tp_buf_len = @truncate(pq.decodeVbyte(buffer.ptr, current_pos));
            // block.term_positions = (try allocator.alignedAlloc(
                // u8,
                // POST_ALIGNMENT,
                // block.tp_buf_len,
            // )).ptr;
            // @memcpy(
                // block.term_positions[0..block.tp_buf_len],
                // buffer[current_pos.*..][0..block.tp_buf_len],
            // );
            // current_pos.* += block.tp_buf_len;
        // }
// 
        // self.partial_block = PostingsBlockPartial.init();
// 
        // self.partial_block.num_docs = buffer[current_pos.*];
        // current_pos.* += 1;
       //  
        // self.partial_block.max_tf       = @truncate(
            // pq.decodeVbyte(buffer.ptr, current_pos)
            // );
        // self.partial_block.max_doc_size = @truncate(
            // pq.decodeVbyte(buffer.ptr, current_pos)
            // );
// 
        // const doc_id_buf_len = pq.decodeVbyte(buffer.ptr, current_pos);
// 
        // try self.partial_block.doc_ids.buffer.resize(allocator, doc_id_buf_len);
// 
        // @memcpy(
            // self.partial_block.doc_ids.buffer.items[0..doc_id_buf_len],
            // buffer[current_pos.*..][0..doc_id_buf_len],
        // );
        // current_pos.* += doc_id_buf_len;
// 
        // const tf_buf_len = pq.decodeVbyte(buffer.ptr, current_pos);
        // try self.partial_block.tfs.buffer.resize(allocator, tf_buf_len);
// 
        // @memcpy(
            // self.partial_block.tfs.buffer.items,
            // buffer[current_pos.*..][0..tf_buf_len],
        // );
        // current_pos.* += tf_buf_len;
// 
        // const tp_buf_len = pq.decodeVbyte(buffer.ptr, current_pos);
        // try self.partial_block.term_positions.resize(allocator, tp_buf_len);
// 
        // @memcpy(
            // self.partial_block.term_positions.items,
            // buffer[current_pos.*..][0..tp_buf_len],
        // );
        // current_pos.* += tp_buf_len;
    // }
// };

pub const PostingsListV2 = struct {
    full_buf_idxs: std.ArrayListUnmanaged(u32),
    partial: std.ArrayListAlignedUnmanaged(
        PostingsBlockPartial,
        POST_ALIGNMENT,
        ),

    full_buffer: []u8,
    full_buffer_idx: u64,


    pub fn init(allocator: std.mem.Allocator) !PostingsListV2 {
        return PostingsListV2{
            .full_buf_idxs = std.ArrayListUnmanaged(u32){},
            .partial = std.ArrayListAlignedUnmanaged(
                PostingsBlockPartial,
                POST_ALIGNMENT,
            ){},

            .full_buffer = try allocator.alloc(
                u8,
                // 1 << 29, // 512MiB
                1 << 30, // 1GiB
            ),
            .full_buffer_idx = 0,
        };
    }

    pub fn deinit(self: *PostingsListV2, allocator: std.mem.Allocator) void {
        self.full_buf_idxs.deinit(allocator);

        allocator.free(self.full_buffer);
        for (self.partial.items) |*p| {
            p.deinit(allocator);
        }
        self.partial.deinit(allocator);
    }

    pub inline fn add(
        self: *PostingsListV2,
        gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        term_id: u32,
        doc_id: u32,
        doc_size: u16,
        term_pos: u16,
        scratch_arr: *align(64) [BLOCK_SIZE]u32,
    ) !void {
        const new_term = term_id == self.partial.items.len;

        if (new_term) {
            const val = try self.partial.addOne(gpa.allocator());
            val.* = PostingsBlockPartial.init();
            try val.add(
                gpa,
                doc_id, 
                doc_size, 
                term_pos, 
                );
            
            try self.full_buf_idxs.append(
                gpa.allocator(), 
                std.math.maxInt(u32),
                );
        } else {
            var partial_block = &self.partial.items[term_id];
            if (
                (partial_block.num_docs == BLOCK_SIZE)
                    and
                (doc_id != partial_block.prev_doc_id)
            ) {

                if (partial_block.num_full_blocks == 0) {
                    self.full_buf_idxs.items[term_id] = @truncate(self.full_buffer_idx);
                }
                try partial_block.flush(
                    gpa,
                    self.full_buffer,
                    &self.full_buffer_idx,
                    scratch_arr,
                    );
            }
            try partial_block.add(
                gpa,
                doc_id,
                doc_size,
                term_pos,
            );
        }
    }

};

// pub const PostingsList = struct {
    // postings: std.ArrayListAlignedUnmanaged(
                  // PostingV3,
                  // POST_ALIGNMENT,
              // ),
    // buffer: []u8,
    // buffer_idx: u64,
// 
    // pub fn init(allocator: std.mem.Allocator) !PostingsList {
        // return PostingsList{
            // .postings = std.ArrayListAlignedUnmanaged(
                  // PostingV3,
                  // POST_ALIGNMENT,
              // ){},
            // .buffer = try allocator.alloc(
                // u8,
                // // 1 << 29, // 512MiB
                // 1 << 30, // 1GiB
            // ),
            // .buffer_idx = 0,
        // };
    // }
// 
    // pub fn deinit(self: *PostingsList, allocator: std.mem.Allocator) void {
        // for (self.postings.items) |*posting| {
            // posting.deinit(allocator);
        // }
        // self.postings.deinit(allocator);
        // allocator.free(self.buffer);
    // }
// 
    // pub inline fn add(
        // self: *PostingsList, 
        // gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        // term_id: u32,
        // doc_id: u32,
        // doc_size: u16,
        // term_pos: u16,
        // scratch_arr: *align(64) [BLOCK_SIZE]u32,
        // ) !void {
        // try self.postings.items[term_id].add(
            // gpa,
            // // &self.arena, 
            // self.buffer, 
            // &self.buffer_idx,
            // doc_id, 
            // doc_size,
            // term_pos,
            // scratch_arr,
            // );
        // std.debug.assert(self.buffer_idx < self.buffer.len);
        // if (self.buffer_idx > 99 * @divFloor(self.buffer.len, 100)) {
            // std.debug.print(
                // "Space Used: {d}/{d}MB\n",
                // .{
                    // self.buffer_idx >> 20,
                    // self.buffer.len >> 20,
                // },
            // );
            // return error.IndexTooLargeAndMergingNotImplemented;
        // }
    // }
// 
    // pub inline fn append(
        // self: *PostingsList, 
        // gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
        // doc_id: u32,
        // doc_size: u16,
        // term_pos: u16,
        // scratch_arr: *align(64) [BLOCK_SIZE]u32,
        // ) !void {
        // const val = try self.postings.addOne(gpa.allocator());
        // val.* = PostingV3.init();
        // try val.add(
            // gpa,
            // // &self.arena,
            // self.buffer,
            // &self.buffer_idx,
            // doc_id, 
            // doc_size, 
            // term_pos, 
            // scratch_arr,
            // );
    // }
// };

pub const InvertedIndexV2 = struct {
    // posting_list: PostingsList,
    // posting_list: PostingsListV2,
    posting_list: PostingsListIndexing,
    posting_list_indexed: PostingsListV2,
    vocab: Vocab,
    doc_sizes: std.ArrayListUnmanaged(u16),

    avg_doc_size: f32,

    file_handle: std.fs.File,

    pub fn init(
        allocator: std.mem.Allocator,
        num_docs: ?usize,
        filename: []const u8,
        create_file: bool,
        ) !InvertedIndexV2 {
        var II = InvertedIndexV2{
            // .posting_list = try PostingsListV2.init(allocator),
            .posting_list = try PostingsListIndexing.init(allocator),
            .posting_list_indexed = try PostingsListV2.init(allocator),
            .vocab = undefined,
            .doc_sizes = std.ArrayListUnmanaged(u16){},
            .avg_doc_size = 0.0,
            .file_handle = undefined,
        };
        if (create_file) {
            II.file_handle = try std.fs.cwd().createFile(
                 filename, 
                 .{ .read = true },
                 );
        } else {
            II.file_handle = try std.fs.cwd().openFile(
                 filename, 
                 .{},
                 );
        }
        II.vocab = Vocab.init();

        if (num_docs) |nd| {
            try II.doc_sizes.ensureTotalCapacity(allocator, nd);

            // Guess capacity
            try II.vocab.string_bytes.ensureTotalCapacity(allocator, @intCast(nd));
            try II.vocab.map.ensureTotalCapacityContext(
                allocator, 
                @intCast(@divFloor(nd, 25)), 
                II.vocab.getCtx(),
                );
        }

        return II;
    }

    pub fn deinit(self: *InvertedIndexV2, allocator: std.mem.Allocator) void {
        self.file_handle.close();

        const start_time_milli = std.time.milliTimestamp();
        self.posting_list.deinit(allocator);
        const end_time_milli = std.time.milliTimestamp();
        std.debug.print(
            "DEINIT TIME: {d}\n",
            .{end_time_milli - start_time_milli},
        );

        // Using arena now.
        self.vocab.deinit(allocator);
        self.doc_sizes.deinit(allocator);
    }

    pub inline fn getDF(self: *InvertedIndexV2, term_id: u32) u32 {
        const partial_block = self.posting_list_indexed.partial.items[term_id];
        return @as(u32, @truncate(partial_block.num_full_blocks * BLOCK_SIZE)) + 
               @as(u32, @intCast(partial_block.num_docs));
    }

    pub fn commit(
        _: *InvertedIndexV2, 
        _: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        _: *std.heap.ArenaAllocator,
        ) !void {}
    // pub fn commit(
        // self: *InvertedIndexV2, 
        // gpa: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        // _: *std.heap.ArenaAllocator,
        // ) !void {
// 
        // var buf = std.ArrayListUnmanaged(u8){};
        // try buf.resize(gpa.allocator(), 1 << 22);
        // defer buf.deinit(gpa.allocator());
// 
        // var current_pos: u64 = 0;
        // std.mem.writePackedInt(
            // u32,
            // buf.items[current_pos..][0..4],
            // 0,
            // @truncate(self.posting_list.partial.items.len),
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
       //  
// 
        // var scratch_arr: [BLOCK_SIZE]u32 align(64) = undefined;
        // for (0.., self.posting_list.partial.items) |term_id, *pos| {
            // if (pos.tp_indexing.items.len > 1) {
                // try pos.flushTPTermBuf(gpa.allocator());
            // }
// 
            // if (pos.num_docs == BLOCK_SIZE) {
                // @branchHint(.unlikely);
// 
                // if (pos.num_full_blocks == 0) {
                    // self.posting_list.full_buf_idxs.items[term_id] = @truncate(
                        // self.posting_list.full_buffer_idx
                        // );
                // }
                // try pos.flush(
                    // gpa,
                    // self.posting_list.full_buffer,
                    // &self.posting_list.full_buffer_idx,
                    // &scratch_arr,
                    // );
            // }
            // try pos.serialize(&buf, gpa.allocator(), &current_pos);
        // }
// 
        // // Directly flush entire full_buffer.
        // var fb_size_buf: [4]u8 = undefined;
        // std.mem.writePackedInt(
            // u32,
            // std.mem.bytesAsSlice(u8, &fb_size_buf),
            // 0,
            // @truncate(self.posting_list.full_buffer_idx),
            // comptime builtin.cpu.arch.endian(),
        // );
        // try self.file_handle.writeAll(std.mem.bytesAsSlice(u8, &fb_size_buf));
        // try self.file_handle.writeAll(
            // self.posting_list.full_buffer[0..self.posting_list.full_buffer_idx],
        // );
        // try self.file_handle.writeAll(
            // std.mem.sliceAsBytes(self.posting_list.full_buf_idxs.items),
        // );
// 
        // std.mem.writePackedInt(
            // u32,
            // buf.items[current_pos..][0..4],
            // 0,
            // @truncate(self.vocab.string_bytes.items.len),
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // if (current_pos + self.vocab.string_bytes.items.len > buf.items.len) {
            // try buf.resize(
                // gpa.allocator(), 
                // buf.items.len + 2 * self.vocab.string_bytes.items.len,
                // );
        // }
        // @memcpy(
            // buf.items[current_pos..][0..self.vocab.string_bytes.items.len],
            // self.vocab.string_bytes.items,
        // );
        // current_pos += self.vocab.string_bytes.items.len;
// 
        // std.mem.writePackedInt(
            // u32,
            // buf.items[current_pos..][0..4],
            // 0,
            // @truncate(self.vocab.map.count()),
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // var max_range = current_pos + (self.vocab.map.count() * 2 * @sizeOf(u32));
        // if (max_range > buf.items.len) {
            // try buf.resize(gpa.allocator(), 2 * max_range);
        // }
        // var map_iterator = self.vocab.map.iterator();
        // while (map_iterator.next()) |item| {
            // std.mem.writePackedInt(
                // u32,
                // buf.items[current_pos..][0..4],
                // 0,
                // item.key_ptr.*,
                // comptime builtin.cpu.arch.endian(),
            // );
            // std.mem.writePackedInt(
                // u32,
                // buf.items[current_pos..][4..8],
                // 0,
                // item.value_ptr.*,
                // comptime builtin.cpu.arch.endian(),
            // );
            // current_pos += 8;
        // }
// 
        // std.mem.writePackedInt(
            // u32,
            // buf.items[current_pos..][0..4],
            // 0,
            // @truncate(self.doc_sizes.items.len),
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // max_range = current_pos + self.doc_sizes.items.len * @sizeOf(u16) + @sizeOf(u32);
        // if (max_range > buf.items.len) {
            // try buf.resize(
                // gpa.allocator(), 
                // max_range,
                // );
        // }
        // @memcpy(
            // buf.items[current_pos..][0..(self.doc_sizes.items.len * @sizeOf(u16))],
            // std.mem.sliceAsBytes(self.doc_sizes.items),
        // );
        // current_pos += self.doc_sizes.items.len * @sizeOf(u16);
// 
        // var ds_sum: u64 = 0;
        // for (self.doc_sizes.items) |ds| {
            // ds_sum += ds;
        // }
// 
        // self.avg_doc_size = @as(f32, @floatFromInt(ds_sum)) / 
                            // @as(f32, @floatFromInt(self.doc_sizes.items.len));
// 
        // std.mem.writePackedInt(
            // u32,
            // buf.items[current_pos..][0..4],
            // 0,
            // @as(u32, @bitCast(self.avg_doc_size)),
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // try self.file_handle.writeAll(
            // buf.items[0..current_pos],
        // );
// 
    // }

    // pub fn load(self: *InvertedIndexV2, allocator: std.mem.Allocator) !void {
    pub fn load(_: *InvertedIndexV2, _: std.mem.Allocator) !void {
        // TODO: Clean up and make work with new structure.
           //  
        // const file_size = try self.file_handle.getEndPos();
        // try self.file_handle.seekTo(0);
// 
        // var buf = try allocator.alloc(u8, file_size);
        // defer allocator.free(buf);
// 
        // const bytes_read = try self.file_handle.readAll(buf);
        // std.debug.assert(bytes_read > 0);
// 
        // var current_pos: usize = 0;
// 
        // // 1. Postings
        // const num_postings = std.mem.readPackedInt(
            // u32,
            // buf[current_pos..][0..4],
            // 0,
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // var arena = std.heap.ArenaAllocator.init(allocator);
// 
        // try self.posting_list.partial.resize(arena.allocator(), num_postings);
        // for (0..num_postings) |idx| {
            // self.posting_list.partial.items[idx] = PostingsBlockPartial.init();
            // try self.posting_list.partial.items[idx].deserialize(
                // arena.allocator(),
                // buf,
                // &current_pos,
            // );
        // }
// 
        // // 2. Vocab
        // const num_string_bytes_vocab = std.mem.readPackedInt(
            // u32,
            // buf[current_pos..][0..4],
            // 0,
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // try self.vocab.string_bytes.resize(allocator, num_string_bytes_vocab);
        // @memcpy(
            // self.vocab.string_bytes.items,
            // buf[current_pos..][0..num_string_bytes_vocab],
        // );
        // current_pos += num_string_bytes_vocab;
// 
        // const num_terms_vocab = std.mem.readPackedInt(
            // u32,
            // buf[current_pos..][0..4],
            // 0,
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // for (0..num_terms_vocab) |_| {
            // const key = std.mem.readPackedInt(
                // u32,
                // buf[current_pos..][0..4],
                // 0,
                // comptime builtin.cpu.arch.endian(),
            // );
            // const value = std.mem.readPackedInt(
                // u32,
                // buf[current_pos..][4..8],
                // 0,
                // comptime builtin.cpu.arch.endian(),
            // );
            // current_pos += 8;
// 
            // try self.vocab.map.putNoClobberContext(
                // allocator, 
                // key, 
                // value, 
                // self.vocab.getCtx(),
                // );
        // }
// 
        // // Doc sizes
        // const num_docs = std.mem.readPackedInt(
            // u32,
            // buf[current_pos..][0..4],
            // 0,
            // comptime builtin.cpu.arch.endian(),
        // );
        // current_pos += 4;
// 
        // try self.doc_sizes.resize(allocator, num_docs);
        // @memcpy(
            // std.mem.sliceAsBytes(self.doc_sizes.items),
            // buf[current_pos..][0..(num_docs * @sizeOf(u16))],
        // );
        // current_pos += num_docs * @sizeOf(u16);
// 
        // // Avg doc size
        // self.avg_doc_size = @bitCast(std.mem.readPackedInt(
            // u32,
            // buf[current_pos..][0..4],
            // 0,
            // comptime builtin.cpu.arch.endian(),
        // ));
        // current_pos += 4;
    }
};

pub const BM25Partition = struct {
    II: []InvertedIndexV2,
    gpa: *std.heap.DebugAllocator(.{.thread_safe = true}),
    arena: std.heap.ArenaAllocator,

    doc_store: DocStore,

    scratch_arr: [BLOCK_SIZE]u32 align(64),

    pub fn init(
        gpa: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        num_search_cols: usize,
        num_records: usize,
        tmp_dir: []const u8,
        partition_idx: usize,
    ) !BM25Partition {
        var partition = BM25Partition{
            .II = try gpa.allocator().alloc(InvertedIndexV2, num_search_cols),
            .gpa = gpa,
            .arena = std.heap.ArenaAllocator.init(std.heap.page_allocator),
            .doc_store = undefined,

            .scratch_arr = undefined,
        };
        @memset(partition.scratch_arr[0..BLOCK_SIZE], 0);

        for (0..num_search_cols) |idx| {
            const output_filename = try std.fmt.allocPrint(
                gpa.allocator(), 
                "{s}/posting_{d}_{d}.bin", 
                .{tmp_dir, idx, partition_idx}
                );
            defer gpa.allocator().free(output_filename);

            partition.II[idx] = try InvertedIndexV2.init(
                partition.gpa.allocator(), 
                num_records,
                output_filename,
                true,
                );
        }

        return partition;
    }

    pub fn deinit(self: *BM25Partition) !void {
        for (0..self.II.len) |i| {
            self.II[i].deinit(self.gpa.allocator());
        }
        self.gpa.allocator().free(self.II);

        try self.doc_store.deinit();
        self.arena.deinit();

    }

    pub fn initFromDisk(
        partition: *BM25Partition,
        gpa: *std.heap.DebugAllocator(.{ .thread_safe = true }),
        dir: []const u8, 
        partition_idx: usize,
        num_search_cols: usize,
        num_cols: usize,
        ) void {

        const allocator = gpa.allocator();
        partition.* = BM25Partition{
            .II = allocator.alloc(InvertedIndexV2, num_search_cols) catch |err| {
                std.debug.print("{any}\n", .{err});
                @panic("Failed to allocate memory for II's");
            },
            .gpa = gpa,
            .arena = std.heap.ArenaAllocator.init(allocator),

            // TODO: Implement doc_store loading.
            .doc_store = undefined,

            .scratch_arr = undefined,
        };
        @memset(partition.scratch_arr[0..BLOCK_SIZE], 0);

        const doc_store_dir = std.fmt.allocPrint(
            allocator,
            "{s}/doc_store",
            .{dir},
            ) catch |err| {
            std.debug.print("{any}\n", .{err});
            @panic("Failed to allocate memory for doc_store_dir");
        };
        defer allocator.free(doc_store_dir);

        partition.doc_store.initFromDisk(
            gpa,
            doc_store_dir, 
            partition_idx,
            num_cols,
            ) catch |err| {
            std.debug.print("{any}\n", .{err});
            @panic("Failed to initialize doc_store from disk");
        };

        for (0..num_search_cols) |idx| {
            const filename = std.fmt.allocPrint(
                allocator,
                "{s}/postings/posting_{d}_{d}.bin",
                .{dir, idx, partition_idx},
                ) catch |err| {
                std.debug.print("{any}\n", .{err});
                @panic("Failed to allocate memory for filename");
            };
            defer allocator.free(filename);

            partition.II[idx] = InvertedIndexV2.init(
                partition.gpa.allocator(), 
                null,
                filename,
                false,
                ) catch |err| {
                std.debug.print("{any}\n", .{err});
                @panic("Failed to initialize InvertedIndexV2 from disk");
            };
            partition.II[idx].load(partition.arena.allocator()) catch |err| {
                std.debug.print("{any}\n", .{err});
                @panic("Failed to load InvertedIndexV2 from disk");
            };
        }
    }

    pub fn resizeNumSearchCols(
        self: *BM25Partition, 
        num_search_cols: usize,
        tmp_dir: []const u8,
        partition_idx: usize,
        ) !void {
        const current_length = self.II.len;
        if (num_search_cols <= current_length) return;


        self.II = try self.gpa.allocator().realloc(self.II, num_search_cols);
        for (current_length..num_search_cols) |idx| {
            const output_filename = try std.fmt.allocPrint(
                self.gpa.allocator(), 
                "{s}/posting_{d}_{d}.bin", 
                .{tmp_dir, idx, partition_idx},
                );
            defer self.gpa.allocator().free(output_filename);

            self.II[idx] = try InvertedIndexV2.init(
                self.gpa.allocator(), 
                self.II[idx].doc_sizes.items.len,
                output_filename,
                true,
                );
        }
    }

    inline fn addTerm(
        self: *BM25Partition,
        term: []u8,
        term_len: usize,
        doc_id: u32,
        term_pos: u16,
        col_idx: usize,
        // scratch_arr: *align(64) [BLOCK_SIZE]u32,
    ) !void {
        std.debug.assert(
            self.II[col_idx].vocab.map.count() < (1 << 32),
            );

        const gop = try self.II[col_idx].vocab.map.getOrPutContextAdapted(
            // self.arena.allocator(),
            self.gpa.allocator(),
            term[0..term_len],
            self.II[col_idx].vocab.getAdapter(),
            self.II[col_idx].vocab.getCtx(),
            );

        self.II[col_idx].doc_sizes.items[doc_id] += 1;

        if (!gop.found_existing) {
            try self.II[col_idx].vocab.string_bytes.appendSlice(
                // self.arena.allocator(),
                self.gpa.allocator(),
                term[0..term_len],
                );
            try self.II[col_idx].vocab.string_bytes.append(
                // self.arena.allocator(), 
                self.gpa.allocator(), 
                0,
                );

            const term_id = self.II[col_idx].vocab.map.count() - 1;
            gop.key_ptr.* = @truncate(
                self.II[col_idx].vocab.string_bytes.items.len - term_len - 1,
                );
            gop.value_ptr.* = term_id;

            try self.II[col_idx].posting_list.add(
                self.gpa,
                term_id,
                doc_id,
                // self.II[col_idx].doc_sizes.items[doc_id],
                term_pos,
                // scratch_arr,
            );

        } else {
            const term_id = gop.value_ptr.*;
            try self.II[col_idx].posting_list.add(
                self.gpa,
                term_id,
                doc_id,
                // self.II[col_idx].doc_sizes.items[doc_id],
                term_pos,
                // scratch_arr,
            );
        }

    }

    inline fn addToken(
        self: *BM25Partition,
        term: []u8,
        cntr: *usize,
        doc_id: u32,
        term_pos: *u16,
        col_idx: usize,
        // scratch_arr: *align(64) [BLOCK_SIZE]u32,
    ) !void {
        if (cntr.* == 0) {
            return;
        }

        try self.addTerm(
            term, 
            cntr.*, 
            doc_id, 
            term_pos.*, 
            col_idx, 
            // scratch_arr,
            );

        term_pos.* += @intFromBool(term_pos.* != std.math.maxInt(u16));
        cntr.* = 0;
    }

    pub fn processDocRfc4180(
        self: *BM25Partition,
        buffer: []u8,
        byte_idx: *usize,
        doc_id: u32,
        col_idx: usize,
    ) !void {
        // std.debug.print(
            // "Capacity Main: {d}MB\n",
            // .{self.arena.queryCapacity() >> 20},
        // );
        try self.II[col_idx].doc_sizes.append(self.arena.allocator(), 0);

        var buffer_idx = byte_idx.*;

        if (
            (buffer[buffer_idx] == ',') 
                or 
            (buffer[buffer_idx] == '\n')
            ) {
            byte_idx.* += 1;
            return;
        }

        var term_pos: u16 = 0;
        const is_quoted = (buffer[buffer_idx] == '"');
        buffer_idx += @intFromBool(is_quoted);

        var cntr: usize = 0;

        if (is_quoted) {

            outer_loop: while (true) {
                if (self.II[col_idx].doc_sizes.items[doc_id] >= MAX_NUM_TERMS) {
                    csv._iterFieldCSV(buffer, byte_idx);
                    return;
                }

                if (cntr > MAX_TERM_LENGTH - 4) {
                    @branchHint(.cold);

                    try self.addToken(
                        buffer[buffer_idx - cntr..], 
                        &cntr, 
                        doc_id, 
                        &term_pos, 
                        col_idx, 
                        // &self.scratch_arr,
                        );
                    buffer_idx += 1;
                    continue;
                }

                switch (buffer[buffer_idx]) {
                    '"' => {
                        buffer_idx += 1;

                        switch (buffer[buffer_idx]) {
                            ',', '\n' => {
                                buffer_idx += 1;
                                break :outer_loop;
                            },
                            '"' => {
                                if (cntr == 0) {
                                    const start_idx = buffer_idx - 1 - @min(buffer_idx - 1, cntr);

                                    try self.addToken(
                                        buffer[start_idx..], 
                                        &cntr, 
                                        doc_id, 
                                        &term_pos, 
                                        col_idx, 
                                        // &self.scratch_arr,
                                        );
                                }
                                buffer_idx += 1;
                                continue;
                            },
                            else => return error.UnexpectedQuote,
                        }
                    },
                    0...33, 35...47, 58...64, 91...96, 123...126 => {
                        if (cntr == 0) {
                            buffer_idx += 1;
                            continue;
                        }

                        const start_idx = buffer_idx - @min(buffer_idx, cntr);

                        try self.addToken(
                            buffer[start_idx..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            col_idx, 
                            // &self.scratch_arr,
                            );
                    },
                    else => {
                        cntr += 1;
                        buffer_idx += 1;
                    },
                }
            }

        } else {

            outer_loop: while (true) {
                std.debug.assert(
                    self.II[col_idx].doc_sizes.items[doc_id] < MAX_NUM_TERMS
                    );

                if (cntr > MAX_TERM_LENGTH - 4) {
                    @branchHint(.cold);
                    try self.addToken(
                        buffer[buffer_idx - cntr..], 
                        &cntr, 
                        doc_id, 
                        &term_pos, 
                        col_idx, 
                        // &self.scratch_arr,
                        );
                    buffer_idx += 1;
                    continue;
                }


                switch (buffer[buffer_idx]) {
                    ',', '\n' => {
                        buffer_idx += 1;
                        break :outer_loop;
                    },
                    0...9, 11...43, 45...47, 58...64, 91...96, 123...126 => {
                        if (cntr == 0) {
                            buffer_idx += 1;
                            cntr = 0;
                            continue;
                        }

                        const start_idx = buffer_idx - @min(buffer_idx, cntr);

                        try self.addToken(
                            buffer[start_idx..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            col_idx, 
                            // &self.scratch_arr,
                            );
                    },
                    else => {
                        cntr       += 1;
                        buffer_idx += 1;
                    },
                }
            }
        }

        if (cntr > 0) {
            std.debug.assert(self.II[col_idx].doc_sizes.items[doc_id] < MAX_NUM_TERMS);

            const start_idx = buffer_idx - @intFromBool(is_quoted) 
                              - @min(buffer_idx - @intFromBool(is_quoted), cntr + 1);

            try self.addToken(
                buffer[start_idx..], 
                &cntr, 
                doc_id, 
                &term_pos, 
                col_idx, 
                // &self.scratch_arr,
                );
        }

        byte_idx.* = buffer_idx;
    }


    pub fn processDocVbyte(
        self: *BM25Partition,
        buffer: []u8,
        doc_id: u32,
        search_col_idx: usize,
    ) !void {
        try self.II[search_col_idx].doc_sizes.append(self.arena.allocator(), 0);

        string_utils.stringToUpper(
            @ptrCast(buffer[0..]), 
            buffer.len,
            );

        if (buffer.len == 0) {
            return;
        }

        var term_pos: u16 = 0;

        var cntr: usize = 0;

        var buffer_idx: usize = 0;
        while (buffer_idx < buffer.len) {
            std.debug.assert(
                self.II[search_col_idx].doc_sizes.items[doc_id] < MAX_NUM_TERMS
                );

            if (cntr > MAX_TERM_LENGTH - 4) {
                @branchHint(.cold);
                try self.addToken(
                    buffer[buffer_idx - cntr..], 
                    &cntr, 
                    doc_id, 
                    &term_pos, 
                    search_col_idx, 
                    // &self.scratch_arr,
                    );
                buffer_idx += 1;
                continue;
            }


            switch (buffer[buffer_idx]) {
                0...47, 58...64, 91...96, 123...126 => {
                    if (cntr == 0) {
                        buffer_idx += 1;
                        cntr = 0;
                        continue;
                    }

                    const start_idx = buffer_idx - @min(buffer_idx, cntr);
                    try self.addToken(
                        buffer[start_idx..], 
                        &cntr, 
                        doc_id, 
                        &term_pos, 
                        search_col_idx, 
                        // &self.scratch_arr,
                        );
                },
                else => {
                    cntr += 1;
                    buffer_idx += 1;
                },
            }
        }

        if (cntr > 0) {
            std.debug.assert(
                self.II[search_col_idx].doc_sizes.items[doc_id] < MAX_NUM_TERMS
                );

            const start_idx = buffer_idx - @min(buffer_idx, cntr + 1);
            try self.addToken(
                buffer[start_idx..], 
                &cntr, 
                doc_id, 
                &term_pos, 
                search_col_idx, 
                // &self.scratch_arr,
                );
        }
    }

    pub fn processDocRfc8259(
        self: *BM25Partition,
        buffer: []u8,
        trie: *const RadixTrie(u32),
        search_col_idxs: *const std.ArrayListUnmanaged(u32),
        byte_idx: *usize,
        doc_id: u32,

        matched_col_idx: *u32,
        value_start_pos: *u64,
        value_end_pos: *u64,
    ) !void {
        // Matches key against keys stored in trie. If not found 
        // maxInt(u32) is returned. If EOL an error is returned.
        // byte_idx is incremented to the next key, next line,
        // or if matched, start of value.

        var cntr: usize = 0;

        var start_key_idx: usize = undefined;
        var end_key_idx:   usize = undefined;
        json.nextKeyValuePoints(
            buffer,
            byte_idx,

            &start_key_idx,
            &end_key_idx,

            value_start_pos,
            value_end_pos,
        );
        const next_key_idx = byte_idx.*;

        var i = value_start_pos.*;
        matched_col_idx.* = json.matchKey(
            buffer,
            &start_key_idx,
            trie,
            true,
        ) catch |err| {
            std.debug.print(
                "Key {s} not found in map.\n", 
                .{buffer[start_key_idx..end_key_idx]},
                );
            return err;
        };

        const II_idx = findSorted(
            u32,
            search_col_idxs.items,
            matched_col_idx.*,
        ) catch |err| {
            if (err == error.KeyNotFound) return;
            return err;
        };

        try self.II[II_idx].doc_sizes.append(self.arena.allocator(), 0);

        var term_pos: u16 = 0;

        // Now we are on the start of a value for a search col key.
        if (buffer[i] == '"') {
            i += 1;

            loop: while (true) {
                if (cntr > MAX_TERM_LENGTH - 4) {
                    @branchHint(.cold);
                    try self.addToken(
                        buffer[i - cntr..], 
                        &cntr, 
                        doc_id, 
                        &term_pos, 
                        II_idx, 
                        // &self.scratch_arr,
                        );
                    i += 1;
                    continue;
                }

                switch (buffer[i]) {
                    '"' => {
                        if (cntr > 0) {
                            const start_idx = i - @min(i, cntr);
                            try self.addToken(
                                buffer[start_idx..], 
                                &cntr, 
                                doc_id, 
                                &term_pos, 
                                II_idx, 
                                // &self.scratch_arr,
                                );
                        }
                        i += 1;
                        break :loop;
                    },
                    0...33, 35...47, 58...64, 91, 93...96, 123...126 => {
                        if (cntr == 0) {
                            i += 1;
                            continue :loop;
                        }

                        const start_idx = i - @min(i, cntr);

                        try self.addToken(
                            buffer[start_idx..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            II_idx, 
                            // &self.scratch_arr,
                            );
                        cntr = 0;

                        if (self.II[II_idx].doc_sizes.items[doc_id] >= MAX_NUM_TERMS) {
                            @branchHint(.cold);
                            break: loop;
                        }

                    },
                    '\\' => i += 2,
                    else => {
                        cntr += 1;
                        i += 1;
                    },
                }
            }
        } else {
            loop: while (true) {
                if (cntr > MAX_TERM_LENGTH - 4) {
                    @branchHint(.cold);
                    try self.addToken(
                        buffer[i - cntr..], 
                        &cntr, 
                        doc_id, 
                        &term_pos, 
                        II_idx, 
                        // &self.scratch_arr,
                        );
                    i += 1;
                    continue;
                }

                switch (buffer[i]) {
                    ',', '}' => break :loop,

                    'N', 'n' => {
                        // NULL
                        i += 4;
                        cntr = 4;

                        try self.addToken(
                            buffer[i - 4..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            II_idx, 
                            // &self.scratch_arr,
                            );
                        break: loop;
                    },
                    'T', 't' => {
                        // TRUE
                        i += 4;
                        cntr = 4;

                        try self.addToken(
                            buffer[i - 4..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            II_idx, 
                            // &self.scratch_arr,
                            );
                        break: loop;
                    },
                    'F', 'f' => {
                        // FALSE
                        i += 5;
                        cntr = 5;

                        try self.addToken(
                            buffer[i - 5..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            II_idx, 
                            // &self.scratch_arr,
                            );
                        break: loop;
                    },
                    '-', '0'...'9' => {
                        cntr = 0;

                        while (
                            (buffer[i] == '-')
                                or
                            (
                             (buffer[i] >= '0')
                                and
                             (buffer[i] <= '9')
                            )
                        ) {
                            i += 1;
                            cntr += 1;
                        }

                        const start_idx = i - @min(i, cntr);

                        try self.addToken(
                            buffer[start_idx..], 
                            &cntr, 
                            doc_id, 
                            &term_pos, 
                            II_idx, 
                            // &self.scratch_arr,
                            );
                        break :loop;
                    },
                    else => {
                        cntr += 1;
                        i += 1;
                    },
                }
            }
        }
        byte_idx.* = next_key_idx;
    }

    pub inline fn fetchRecordsDocStore(
        self: *BM25Partition,
        result_positions: []TermPos,
        query_result: QueryResult,
        record_string: *std.ArrayListUnmanaged(u8),

        bit_sizes: []u32,
    ) void {
        self.doc_store.getRow(
            @intCast(query_result.doc_id),
            record_string,
            self.gpa.allocator(),
            result_positions,

            bit_sizes,
        ) catch {
            @panic("Error fetching document.");
        };
    }
};
